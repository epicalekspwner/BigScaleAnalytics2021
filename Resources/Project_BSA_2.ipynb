{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project BSA 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6NkxI7wXxRkd",
        "Cj85tYWx2sYN",
        "rbu_NYFn28O6",
        "5CRTnDOI34h_",
        "CYb1OefzVjMD",
        "2xM1fQFU-Qtz",
        "Df6emro998x9"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NkxI7wXxRkd"
      },
      "source": [
        "# Import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuOFs-TEZlar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb26f80e-c46f-463a-93af-891d7ba4da82"
      },
      "source": [
        "!pip install -U spacy\n",
        "! pip install nltk\n",
        "!python -m spacy download fr_core_news_md\n",
        "!pip install spacy-lefff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 288kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 42.5MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 21.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/de/68/027c9f70a58fa4d76521d94237e305247fca196d374635b339401ebed5d8/catalogue-2.0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=27b3fee6fc48b0a127a313d70cbb7f998dba40652da45b574dc043acd18e8a5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "\u001b[31mERROR: catalogue 2.0.2 has requirement importlib-metadata<3.3.0,>=0.20; python_version < \"3.8\", but you'll have importlib-metadata 3.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pydantic, catalogue, srsly, smart-open, typer, pathy, thinc, spacy-legacy, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.2 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 srsly-2.4.1 thinc-8.0.2 typer-0.3.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "2021-04-14 16:44:13.303747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting fr-core-news-md==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.0.0/fr_core_news_md-3.0.0-py3-none-any.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 98kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.10.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.24.3)\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n",
            "Collecting spacy-lefff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/21/a326ed06c4175ec07baf61ee563dbc5bc06637e5d863f11ce9409d007203/spacy-lefff-0.4.0.tar.gz (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 4.6MB/s \n",
            "\u001b[?25hCollecting spacy<3.0.5,>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/c4/a5a8aa936e1b7fbba1780863447a51684dabb7f8855931f2510d4637d641/spacy-3.0.4-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 19.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.3.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.7.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.11.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.10.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.19.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (8.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (4.41.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (20.9)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.7.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (54.2.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.0.2)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.24.3)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.4.7)\n",
            "Building wheels for collected packages: spacy-lefff\n",
            "  Building wheel for spacy-lefff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lefff: filename=spacy_lefff-0.4.0-cp37-none-any.whl size=2929893 sha256=cb443ba62840ae76be8ea16c257d6b5f1bb5e9f4320a3f31f96bed05ea915b15\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/1c/f0/9b95e4e74005afbfe54aa126484febacf0fd27feffa3e9ad45\n",
            "Successfully built spacy-lefff\n",
            "Installing collected packages: spacy, spacy-lefff\n",
            "  Found existing installation: spacy 3.0.5\n",
            "    Uninstalling spacy-3.0.5:\n",
            "      Successfully uninstalled spacy-3.0.5\n",
            "Successfully installed spacy-3.0.4 spacy-lefff-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFLZfk2kxL_A"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import collections\n",
        "from collections import Counter\n",
        "import string\n",
        "punctuations = string.punctuation\n",
        "import re\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "from spacy_lefff import LefffLemmatizer\n",
        "from spacy.language import Language\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_md\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfoaNi92zEaY"
      },
      "source": [
        "Phrases = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Phrasee.txt?token=ARJRH4TY7YDNNLHDR73ZUMDAQBLL2', delimiter = \"\\t\")\n",
        "Niveau = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Niveau.txt?token=ARJRH4UJHAWR3R2MEP44EQ3AQBLNG', delimiter = \"\\t\")\n",
        "df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj85tYWx2sYN"
      },
      "source": [
        "# Merge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7FINW7ozYcX"
      },
      "source": [
        "df_merge = pd.concat([Phrases, Niveau], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj2IkgoUQ331"
      },
      "source": [
        "df_complet = df_merge.rename(columns={'Type (Maxime)': 'Niveau'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbu_NYFn28O6"
      },
      "source": [
        "#Separate dataset by level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pJWP60r3Iti"
      },
      "source": [
        "df_A1 = df_complet[df_complet[\"Niveau\"] == \"A1\"]\n",
        "df_A2 = df_complet[df_complet[\"Niveau\"] == \"A2\"]\n",
        "df_B1 = df_complet[df_complet[\"Niveau\"] == \"B1\"]\n",
        "df_B2 = df_complet[df_complet[\"Niveau\"] == \"A2\"]\n",
        "df_C1 = df_complet[df_complet[\"Niveau\"] == \"C1\"]\n",
        "df_C2 = df_complet[df_complet[\"Niveau\"] == \"C2\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snyODphn3xIZ"
      },
      "source": [
        "# Data cleaning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CRTnDOI34h_"
      },
      "source": [
        "## Remove ponctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CPJ4TOOAGCH"
      },
      "source": [
        "def RemovePonct (text):\n",
        "  my_token = ''\n",
        "  for i in text:\n",
        "   my_token += str(i)\n",
        "  newtext = my_token\n",
        "  clean = re.sub(r\"\"\"\n",
        "               [,.;@#?!&$']+  # Accept one or more copies of punctuation\n",
        "               \\ *           # plus zero or more copies of a space,\n",
        "               \"\"\",\n",
        "               \" \",          # and replace it with a single space\n",
        "               newtext, flags=re.VERBOSE)\n",
        "  return clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DrrH7FAAmwi"
      },
      "source": [
        "A1clean1 = df_A1['Phrases'].apply(RemovePonct)\n",
        "A2clean1 = df_A2['Phrases'].apply(RemovePonct)\n",
        "B1clean1 = df_B1['Phrases'].apply(RemovePonct)\n",
        "B2clean1 = df_B2['Phrases'].apply(RemovePonct)\n",
        "C1clean1 = df_C1['Phrases'].apply(RemovePonct)\n",
        "C2clean1 = df_C2['Phrases'].apply(RemovePonct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYb1OefzVjMD"
      },
      "source": [
        "## Remove upper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ0NwiqSVMl7"
      },
      "source": [
        "A1clean2 = A1clean1.str.lower()\n",
        "A2clean2 = A2clean1.str.lower()\n",
        "B1clean2 = B1clean1.str.lower()\n",
        "B2clean2 = B2clean1.str.lower()\n",
        "C1clean2 = C1clean1.str.lower()\n",
        "C2clean2 = C2clean1.str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xM1fQFU-Qtz"
      },
      "source": [
        "## Remove pronouns and determiners"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JOsqGFe-jRy"
      },
      "source": [
        "#à garder: NOUN ADJ VERB\n",
        "def POS_french_token(sentences):\n",
        "    dict1 = {}\n",
        "    doc = nlp(sentences)\n",
        "    for token in doc:\n",
        "      dict1.update({token.text : token.pos_})\n",
        "    return dict1\n",
        "\n",
        "A1clean3 = A1clean2.apply(lambda x: POS_french_token(x))\n",
        "A2clean3 = A2clean2.apply(lambda x: POS_french_token(x))\n",
        "B1clean3 = B1clean2.apply(lambda x: POS_french_token(x))\n",
        "B2clean3 = B2clean2.apply(lambda x: POS_french_token(x))\n",
        "C1clean3 = C1clean2.apply(lambda x: POS_french_token(x))\n",
        "C2clean3 = C2clean2.apply(lambda x: POS_french_token(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey85ijbfS_Ik",
        "outputId": "ad628ddd-7e39-42dd-a8aa-a46c077ce93a"
      },
      "source": [
        "A1clean3[19]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 'SPACE',\n",
              " 'journalistes': 'NOUN',\n",
              " 'les': 'DET',\n",
              " 'prêts': 'ADJ',\n",
              " 'sont': 'AUX',\n",
              " 'tout': 'DET',\n",
              " 'à': 'ADP'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVQe9Gonss21"
      },
      "source": [
        "#je n'arrive pas, je ne comprend pas comment garder que les noms, verbes, etc.\n",
        "#et après avoir remove, remettre en series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df6emro998x9"
      },
      "source": [
        "## Tokenization and count most freq words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9vXh1aIQcHg"
      },
      "source": [
        "                                                               #j'arrive pas à faire fonctionner en fonction\n",
        "#def TokenizAndCount (text):\n",
        "#    my_doc = nlp(text)\n",
        "#    token_list = []\n",
        "#    for token in my_doc:\n",
        "#      token_list.append(token.text)\n",
        "#    occurrences = collections.Counter(token_list)\n",
        "#    most_freq_words = occurrences.most_common()\n",
        "#    return most_freq_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXVDuXAwAAF_"
      },
      "source": [
        "#A1cleanFinal = A1clean2.apply(TokenizAndCount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUC32Q4wtG8W"
      },
      "source": [
        "#après avoir fait fonctionné la fonction, essayer d'enlever les mots apparaissants dans A1, de A2, et ainsi de suite ... pour avoir les top exclusif au niveau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT8AfRBao21H"
      },
      "source": [
        "### solution temporaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DiL3MFRoUKt",
        "outputId": "e179be7a-54f1-461f-dcbc-75c168291ce3"
      },
      "source": [
        "                                                               #solution temporaire A1\n",
        "textA1 = A1clean2\n",
        "my_tokenA1 = ''\n",
        "for i in textA1:\n",
        "   my_tokenA1 += str(i)\n",
        "textA1 = my_tokenA1\n",
        "my_doc = nlp(textA1)\n",
        "token_listA1 = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_listA1.append(token.text)\n",
        "token_listA1\n",
        "occurrencesA1 = collections.Counter(token_listA1)\n",
        "most_freq_wordsA1 = occurrencesA1.most_common(10)\n",
        "most_freq_wordsA1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 33),\n",
              " ('je', 30),\n",
              " (' ', 29),\n",
              " ('la', 29),\n",
              " ('est', 28),\n",
              " ('et', 27),\n",
              " ('le', 26),\n",
              " ('il', 25),\n",
              " ('à', 19),\n",
              " ('un', 18)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_czy_p_p0-u",
        "outputId": "d0e322f6-a5e9-430f-bfdb-a3a043fce30a"
      },
      "source": [
        "                                                               #solution temporaire A2\n",
        "textA2 = A2clean2\n",
        "my_tokenA2 = ''\n",
        "for i in textA2:\n",
        "   my_tokenA2 += str(i)\n",
        "textA2 = my_tokenA2\n",
        "my_doc = nlp(textA2)\n",
        "token_listA2 = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_listA2.append(token.text)\n",
        "token_listA2\n",
        "occurrencesA2 = collections.Counter(token_listA2)\n",
        "most_freq_wordsA2 = occurrencesA2.most_common(10)\n",
        "most_freq_wordsA2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 78),\n",
              " ('la', 60),\n",
              " ('et', 53),\n",
              " ('le', 48),\n",
              " ('les', 45),\n",
              " ('est', 40),\n",
              " ('il', 39),\n",
              " ('un', 34),\n",
              " ('je', 34),\n",
              " ('à', 33)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM5f4IQop21z",
        "outputId": "d96f25a5-3fb6-4288-de65-e2bbf67e0719"
      },
      "source": [
        "                                                               #solution temporaire B1\n",
        "textB1 = B1clean2\n",
        "my_tokenB1 = ''\n",
        "for i in textB1:\n",
        "   my_tokenB1 += str(i)\n",
        "textB1 = my_tokenB1\n",
        "my_doc = nlp(textB1)\n",
        "token_listB1 = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_listB1.append(token.text)\n",
        "token_listB1\n",
        "occurrencesB1 = collections.Counter(token_listB1)\n",
        "most_freq_wordsB1 = occurrencesB1.most_common(10)\n",
        "most_freq_wordsB1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 88),\n",
              " ('la', 56),\n",
              " ('un', 50),\n",
              " ('les', 49),\n",
              " ('et', 48),\n",
              " ('le', 45),\n",
              " ('à', 42),\n",
              " ('elle', 38),\n",
              " ('je', 37),\n",
              " ('des', 35)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r0IlL9kp3ts",
        "outputId": "ebad6b1e-4d6f-4b05-e681-2231288e7845"
      },
      "source": [
        "                                                               #solution temporaire B2\n",
        "textB2 = B2clean2\n",
        "my_tokenB2 = ''\n",
        "for i in textB2:\n",
        "   my_tokenB2 += str(i)\n",
        "textB2 = my_tokenB2\n",
        "my_doc = nlp(textB2)\n",
        "token_listB2 = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_listB2.append(token.text)\n",
        "token_listB2\n",
        "occurrences = collections.Counter(token_listB2)\n",
        "most_freq_wordsB2 = occurrences.most_common(10)\n",
        "most_freq_wordsB2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 78),\n",
              " ('la', 60),\n",
              " ('et', 53),\n",
              " ('le', 48),\n",
              " ('les', 45),\n",
              " ('est', 40),\n",
              " ('il', 39),\n",
              " ('un', 34),\n",
              " ('je', 34),\n",
              " ('à', 33)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tynLyzyp4Qb",
        "outputId": "5ce4d45f-ba08-450c-8795-19caed4a72cd"
      },
      "source": [
        "                                                               #solution temporaire C1\n",
        "textC1 = C1clean2\n",
        "my_tokenC1 = ''\n",
        "for i in textC1:\n",
        "   my_tokenC1 += str(i)\n",
        "textC1 = my_tokenC1\n",
        "my_doc = nlp(textC1)\n",
        "token_listC1 = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_listC1.append(token.text)\n",
        "token_listC1\n",
        "occurrencesC1 = collections.Counter(token_listC1)\n",
        "most_freq_wordsC1 = occurrencesC1.most_common(10)\n",
        "most_freq_wordsC1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 274),\n",
              " ('la', 168),\n",
              " ('les', 144),\n",
              " ('le', 126),\n",
              " ('et', 114),\n",
              " ('à', 113),\n",
              " ('des', 96),\n",
              " ('une', 72),\n",
              " ('d’', 69),\n",
              " ('un', 59)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X74Guk1Up4vk",
        "outputId": "56678462-a337-4416-8ada-359c341c47a6"
      },
      "source": [
        "                                                               #solution temporaire C2\n",
        "textC2 = C2clean2\n",
        "my_tokenC2 = ''\n",
        "for i in textC2:\n",
        "   my_tokenC2 += str(i)\n",
        "textC2 = my_tokenC2\n",
        "my_doc = nlp(textC2)\n",
        "token_listC2 = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_listC2.append(token.text)\n",
        "token_listC2\n",
        "occurrencesC2 = collections.Counter(token_listC2)\n",
        "most_freq_wordsC2 = occurrencesC2.most_common(10)\n",
        "most_freq_wordsC2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('de', 772),\n",
              " ('la', 404),\n",
              " ('et', 359),\n",
              " ('à', 309),\n",
              " ('les', 279),\n",
              " ('un', 244),\n",
              " ('le', 242),\n",
              " ('en', 224),\n",
              " ('des', 205),\n",
              " ('une', 195)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}