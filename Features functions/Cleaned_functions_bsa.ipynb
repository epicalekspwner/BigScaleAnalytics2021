{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaned_functions_bsa.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UoqIGtABJYwc"
      ],
      "authorship_tag": "ABX9TyOEogNphLlm64nL0EpQz7Ta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epicalekspwner/BigScaleAnalytics2021/blob/main/Cleaned_functions_bsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoqIGtABJYwc"
      },
      "source": [
        "## 1/Libraries and lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wz0bXwZBdky"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from collections import OrderedDict\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rr9GaNqtBN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5785d9-ff69-4f14-eaa5-922f6a4689b0"
      },
      "source": [
        "pip install spacy-lefff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-lefff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/21/a326ed06c4175ec07baf61ee563dbc5bc06637e5d863f11ce9409d007203/spacy-lefff-0.4.0.tar.gz (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.5MB/s \n",
            "\u001b[?25hCollecting spacy<3.0.5,>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/c4/a5a8aa936e1b7fbba1780863447a51684dabb7f8855931f2510d4637d641/spacy-3.0.4-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 25.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.8.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.7.4.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (54.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (4.41.1)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.0.5)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 45.9MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.8.2)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.19.5)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (7.1.2)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.4)\n",
            "Building wheels for collected packages: spacy-lefff, smart-open\n",
            "  Building wheel for spacy-lefff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lefff: filename=spacy_lefff-0.4.0-cp37-none-any.whl size=2929893 sha256=5ad23026de1c6d778cd9fe5e20970c3595aa686cc6a4484fa4a43d1f580d3931\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/1c/f0/9b95e4e74005afbfe54aa126484febacf0fd27feffa3e9ad45\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=daa70fe4524de63ac7a8c4516173b425a7f579b8fc9e0a13a08b893cc8b34e98\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built spacy-lefff smart-open\n",
            "Installing collected packages: catalogue, typer, pydantic, spacy-legacy, smart-open, pathy, srsly, thinc, spacy, spacy-lefff\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.4 spacy-lefff-0.4.0 spacy-legacy-3.0.2 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak_u5r4kBnwS",
        "outputId": "07ba2813-910a-45e4-aabb-d107b08fe9dc"
      },
      "source": [
        "# Spacy library\n",
        "\n",
        "!python -m spacy download fr_core_news_md\n",
        "\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import spacy\n",
        "from spacy_lefff import LefffLemmatizer\n",
        "from spacy.language import Language\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_md\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-12 06:53:08.842068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting fr-core-news-md==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.0.0/fr_core_news_md-3.0.0-py3-none-any.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 97kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.0)\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LJL0FB9Sog5"
      },
      "source": [
        "# NLTK --> Stemming\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmerfr = SnowballStemmer(\"french\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppj2_EpVSvmF",
        "outputId": "a04b66c7-05be-496d-e0b4-671b4d4fb498"
      },
      "source": [
        "pip install deep-translator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b3/53b50057bf1a5ebbb8a77c19c42ae721129ebe1b8478029f2121a5914f07/deep_translator-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (4.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (2020.12.5)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNfQH0hMTqw2"
      },
      "source": [
        "from deep_translator import GoogleTranslator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bQGWOImcHij"
      },
      "source": [
        "# Special lists/dict of char\n",
        "\n",
        "liste_special_letters = ['é','è','ê','ë','à','ä','â','ï','î','ö','ô','ü','ù','û','ç']\n",
        "contractions_dict = { \"d'\": \"de \",\"n'\":\"ne \",\"l'\":\"le \",\"s'\":\"se \",\"c'\":\"ce \",\"j'\":\"je \",\"t'\":\"tu \",\"qu'\":\"que \"}\n",
        "stopWord = ['le','la','les','de','des','se','ce','à']\n",
        "Fr_stp_modif = {'a', 'abord', 'afin', 'ah', 'ai', 'aie', 'ainsi',\n",
        " 'alors', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', \n",
        " 'assez',  'au', 'aucun', 'aucune', 'aujourd', \"aujourd'hui\", 'aupres',\n",
        " 'auquel', 'aussi', 'autre', 'autrement', 'autres', 'autrui',\n",
        " 'aux',  'auxquelles', 'auxquels',  'avant', 'avec',  'bas', 'basee', 'car', 'ce', 'ceci',\n",
        " 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', 'celui',\n",
        " 'celui-ci', 'celui-là', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', 'cet', 'cette', 'ceux',\n",
        " 'ceux ci', 'ceux là', 'chacun', 'chacune', 'chaque', 'chez', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième',\n",
        " 'combien', 'comme', 'comment', 'compris', 'concernant', 'da', 'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis',\n",
        " 'derriere', 'derrière', 'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant',\n",
        " 'devers', 'devra', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', 'directe',\n",
        " 'directement', 'divers', 'diverse', 'diverses', 'dix', 'dix huit', 'dix neuf', 'dix sept', 'dixième', 'donc', 'dont', 'douze',\n",
        " 'douzième', 'du', 'duquel', 'durant','dès', 'désormais', 'effet','egale', 'egalement', 'egales', 'eh', 'elle', 'elle même',\n",
        " 'elles', 'elles mêmes', 'en', 'encore', 'enfin', 'entre','envers', 'environ','et', 'etc', 'eux', 'eux mêmes',\n",
        " 'exactement',\n",
        " 'excepté',\n",
        "  'chat'\n",
        "  'chien'\n",
        " 'façon',\n",
        "\n",
        " 'gens',\n",
        " 'ha',\n",
        " 'hem',\n",
        " 'hep',\n",
        " 'hi',\n",
        " 'ho',\n",
        " 'hormis',\n",
        " 'hors',\n",
        " 'hou',\n",
        " 'houp',\n",
        " 'hue',\n",
        " 'hui',\n",
        " 'huit',\n",
        " 'huitième',\n",
        " 'hé',\n",
        " 'i',\n",
        " 'il',\n",
        " 'ils',\n",
        " 'importe',\n",
        " \"j \",\n",
        " 'je',\n",
        " 'jusqu',\n",
        " 'jusque',\n",
        " 'juste',\n",
        " 'j ',\n",
        " \"l \",\n",
        " 'la',\n",
        " 'laquelle',\n",
        " 'le',\n",
        " 'lequel',\n",
        " 'les',\n",
        " 'lesquelles',\n",
        " 'lesquels',\n",
        " 'leur',\n",
        " 'leurs',\n",
        " 'longtemps',\n",
        " 'lors',\n",
        " 'lorsque',\n",
        " 'lui',\n",
        " 'lui meme',\n",
        " 'lui même',\n",
        " 'là',\n",
        " 'lès',\n",
        " 'l ',\n",
        " \"m \",\n",
        " 'ma',\n",
        " 'maint',\n",
        " 'maintenant',\n",
        " 'mais',\n",
        " 'malgré',\n",
        " 'me',\n",
        " 'meme',\n",
        " 'memes',\n",
        " 'merci',\n",
        " 'mes',\n",
        " 'mien',\n",
        " 'mienne',\n",
        " 'miennes',\n",
        " 'miens',\n",
        " 'mille',\n",
        " 'moi',\n",
        " 'moi meme',\n",
        " 'moi même',\n",
        " 'moindres',\n",
        " 'moins',\n",
        " 'mon',\n",
        " 'même',\n",
        " 'mêmes',\n",
        " 'm ',\n",
        " \"n \",\n",
        " 'na',\n",
        " 'ne',\n",
        " 'neanmoins',\n",
        " 'neuvième',\n",
        " 'ni',\n",
        " 'nombreuses',\n",
        " 'nombreux',\n",
        " 'nos',\n",
        " 'notamment',\n",
        " 'notre',\n",
        " 'nous',\n",
        " 'nous-mêmes',\n",
        " 'nouvea',\n",
        " 'nul',\n",
        " 'néanmoins',\n",
        " 'nôtre',\n",
        " 'nôtres',\n",
        " 'n’',\n",
        " 'o',\n",
        " 'on',\n",
        " \n",
        " 'onze',\n",
        " 'onzième',\n",
        " 'ore',\n",
        " 'ou',\n",
        " 'ouias',\n",
        " 'oust',\n",
        " 'outre',\n",
        "\n",
        " 'ouverte',\n",
        " 'ouverts',\n",
        " 'où',\n",
        " 'par',\n",
        " 'parce',\n",
        " 'parfois',\n",
        " 'parmi',\n",
        " 'parseme',\n",
        " 'partant',\n",
        " 'pas',\n",
        " 'pendant',\n",
        " 'pense',\n",
        " 'permet',\n",
        " 'personne',\n",
        " 'peu',\n",
        "\n",
        " 'plus',\n",
        " 'plusieurs',\n",
        " 'plutôt',\n",
        " 'possible',\n",
        " 'possibles',\n",
        " 'pour',\n",
        " 'pourquoi',\n",
        "\n",
        " 'prealable',\n",
        " 'precisement',\n",
        " 'premier',\n",
        " 'première',\n",
        " 'premièrement',\n",
        " 'pres',\n",
        " 'procedant',\n",
        " 'proche',\n",
        " 'près',\n",
        " 'pu',\n",
        " 'puis',\n",
        " 'puisque',\n",
        " \"qu'\",\n",
        " 'quand',\n",
        " 'quant',\n",
        " 'quant à soi',\n",
        " 'quanta',\n",
        " 'quarante',\n",
        " 'quatorze',\n",
        " 'quatre',\n",
        " 'quatre vingt',\n",
        " 'quatrième',\n",
        " 'quatrièmement',\n",
        " 'que',\n",
        " 'quel',\n",
        " 'quelconque',\n",
        " 'quelle',\n",
        " 'quelles',\n",
        " \"quelqu'un\",\n",
        " 'quelque',\n",
        " 'quelques',\n",
        " 'quels',\n",
        " 'qui',\n",
        " 'quiconque',\n",
        " 'quinze',\n",
        " 'quoi',\n",
        " 'quoique',\n",
        " 'qu’',\n",
        " 'relative',\n",
        " 'relativement',\n",
        " \n",
        "\n",
        " 'retour',\n",
        " 'revoici',\n",
        " 'revoilà',\n",
        " \"s \",\n",
        " 'sa',\n",
        " 'sait',\n",
        " 'sans',\n",
        " 'sauf',\n",
        " 'se',\n",
        " 'seize',\n",
        " 'selon',\n",
        " 'semblable',\n",
        " 'semblaient',\n",
        " 'semble',\n",
        " 'semblent',\n",
        " 'sent',\n",
        " 'sept',\n",
        " 'septième',\n",
        "\n",
        " 'ses',\n",
        " 'seul',\n",
        " 'seule',\n",
        " 'seulement',\n",
        " 'si',\n",
        " 'sien',\n",
        " 'sienne',\n",
        " 'siennes',\n",
        " 'siens',\n",
        " 'sinon',\n",
        " 'six',\n",
        " 'sixième',\n",
        " 'soi',\n",
        " 'soi même',\n",
        " 'soit',\n",
        " 'soixante',\n",
        " 'son',\n",
        "\n",
        " 'sous',\n",
        " 'souvent',\n",
        " 'specifique',\n",
        " 'specifiques',\n",
        " 'stop',\n",
        " 'suffisant',\n",
        " 'suffisante',\n",
        "\n",
        " 'suivant',\n",
        " 'suivante',\n",
        " 'suivantes',\n",
        " 'suivants',\n",
        " 'suivre',\n",
        " 'sur',\n",
        " 'surtout',\n",
        " 's ',\n",
        " \"t \",\n",
        " 'ta',\n",
        " 'tant',\n",
        " 'te',\n",
        " 'tel',\n",
        " 'telle',\n",
        " 'tellement',\n",
        " 'telles',\n",
        " 'tels',\n",
        " 'tenant',\n",
        "\n",
        " 'tente',\n",
        " 'tes',\n",
        " 'tien',\n",
        " 'tienne',\n",
        " 'tiennes',\n",
        " 'tiens',\n",
        " 'toi',\n",
        " 'toi-même',\n",
        " 'ton',\n",
        " 'touchant',\n",
        " 'toujours',\n",
        " 'tous',\n",
        " 'tout',\n",
        " 'toute',\n",
        " 'toutes',\n",
        " 'treize',\n",
        " 'trente',\n",
        " 'tres',\n",
        " 'trois',\n",
        " 'troisième',\n",
        " 'troisièmement',\n",
        " 'tu',\n",
        " 'té',\n",
        " 't ',\n",
        " 'un',\n",
        " 'une',\n",
        " 'unes',\n",
        " 'uns',\n",
        " 'va',\n",
        " 'vais',\n",
        " 'vas',\n",
        " 'vers',\n",
        " 'via',\n",
        " 'vingt',\n",
        " 'voici',\n",
        " 'voilà',\n",
        " 'vont',\n",
        " 'vos',\n",
        " 'votre',\n",
        " 'vous',\n",
        " 'vous mêmes',\n",
        " 'vu',\n",
        " 'vé',\n",
        " 'vôtre',\n",
        " 'vôtres',\n",
        " 'à',\n",
        " 'â',\n",
        " 'ça',\n",
        " 'ès',\n",
        " 'ô'}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b9LBUAlpDQr"
      },
      "source": [
        "## 2/ Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S2fxHFSJhFl"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS2VTNEcJGMV"
      },
      "source": [
        "# Dataframe with french sentences and level\n",
        "\n",
        "Phrases = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Phrasee.txt', delimiter = \"\\t\")\n",
        "Niveau = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Niveau.txt', delimiter = \"\\t\" )\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['Level'] = Niveau['Type (Maxime)']\n",
        "df['Sentences'] = Phrases['Phrases']\n",
        "\n",
        "#NOTE:to apply these functions make sure that the language level are called \"Level\" and the sentences \"Sentences\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqDwMaoAaz2P"
      },
      "source": [
        "def cleaning_general(df):\n",
        "\n",
        "    # Data cleaning 1 --> char replacement + lower\n",
        "\n",
        "    df['Sentences'] = df['Sentences'].apply(lambda x: x.replace(\"’\",\"'\"))\n",
        "    df['Sentences'] = df['Sentences'].apply(lambda x: x.lower())\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 2 --> Contractions expanding\n",
        "\n",
        "    contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "        # Function for expanding contractions\n",
        "\n",
        "    def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "      def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "      return contractions_re.sub(replace, text)\n",
        "\n",
        "    df['Cleaning1'] = df['Sentences'].apply(lambda x: expand_contractions(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 3 --> Punctuation removal\n",
        "\n",
        "    def punctuation_removal(text):\n",
        "        all_list = [char for char in text if char not in string.punctuation]\n",
        "        clean_str = ''.join(all_list)\n",
        "        return clean_str\n",
        "\n",
        "    df['Cleaning2'] = df['Cleaning1'].apply(lambda x: punctuation_removal(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 4 --> Remove special char (took into account french special char e.g. \"ç\",\"é\")\n",
        "\n",
        "    def bin_spe(tweet):\n",
        "        tweet = ' '.join(re.sub(\"[^0-9a-zÀ-ÿ-A-Z-ç \\t]\",\" \", tweet).split())\n",
        "        return tweet\n",
        "\n",
        "    df['Cleaning3'] = df['Cleaning2'].apply(lambda x: bin_spe(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 5 --> Stop words removal\n",
        "\n",
        "    def stopword_removal(text):\n",
        "        all_list = [char for char in text if char not in stopWord]\n",
        "        clean_str = ''.join(all_list)\n",
        "        return clean_str\n",
        "\n",
        "    df['Cleaning3'] = df['Cleaning2'].apply(lambda x: stopword_removal(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 6 --> remove alone letter and words with len() ==2\n",
        "\n",
        "    def remove_alone_letters_and_len2(texte):\n",
        "        texte = ' '.join(i for i in texte.split() if not len(i) == 1 and not len(i) == 2)\n",
        "        return texte\n",
        "\n",
        "    df['Cleaning4'] = df['Cleaning3'].apply(lambda x: remove_alone_letters_and_len2(x))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhMTFpyObspN"
      },
      "source": [
        "def lemma_and_Leff(df1):\n",
        "    \n",
        "    \n",
        "    # Count top 100 words whole dataset\n",
        "\n",
        "    Counter(\" \".join(df1[\"Cleaning4\"]).split()).most_common(100)\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Lemmatization --> OUTPUT: Token\n",
        "\n",
        "    def lemma_french_token(text):\n",
        "      doc = nlp(text)\n",
        "      liste1 = []\n",
        "      for token in doc:\n",
        "        liste1.append(token.lemma_)\n",
        "      return liste1\n",
        "\n",
        "    df1['lemma'] = df1['Cleaning4'].apply(lambda x: lemma_french_token(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Lemmatization --> OUTPUT: sentence\n",
        "\n",
        "    def lemma_french_full_sentence(text):\n",
        "      doc = nlp(text)\n",
        "      lemmatized_output = ' '.join([w.lemma_ for w in doc])\n",
        "      return lemmatized_output\n",
        "\n",
        "    df1['lemma'] = df1['Cleaning4'].apply(lambda x: lemma_french_full_sentence(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Count to 100 words after lemma whole dataset\n",
        "\n",
        "    Top_100_lemma = Counter(\" \".join(df1[\"lemma\"]).split()).most_common(100)\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Get a list of the top 100 words in the dataset\n",
        "\n",
        "    def count_to_list(my_list_count_100):\n",
        "      list2 = []\n",
        "      for my_tuple in my_list_count_100:\n",
        "          list2.append(my_tuple[0])\n",
        "      return list2\n",
        "\n",
        "    List_top_100 = count_to_list(Top_100_lemma)\n",
        "\n",
        "\n",
        "    List_top_100\n",
        "\n",
        "    # Data cleaning 7 --> Common word removal\n",
        "\n",
        "    def common_word_removal(text):\n",
        "\n",
        "        all_list =  \" \".join([word for word in text.split() if word not in List_top_100])\n",
        "        \n",
        "        return all_list\n",
        "\n",
        "    df1['Cleaning7'] = df1['lemma'].apply(lambda x: common_word_removal(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def List_STP_word_removal(text):\n",
        "\n",
        "        all_list =  \" \".join([word for word in text.split() if word not in Fr_stp_modif])\n",
        "        \n",
        "        return all_list\n",
        "\n",
        "    df1['Cleaning8'] = df1['Cleaning7'].apply(lambda x: List_STP_word_removal(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#Create dict that returns the type of the word as value and as key the word {\"word\" : \"Type\"}\n",
        "\n",
        "    def POS_french_token(sentences):\n",
        "        dict1 = {}\n",
        "        doc = nlp(sentences)\n",
        "        for token in doc:\n",
        "          dict1.update({token.text : token.pos_})\n",
        "        return dict1\n",
        "\n",
        "    df1['POS_LEFFF2'] = df1['lemma'].apply(lambda x: POS_french_token(x))\n",
        "    df1['POS_without_lemma'] = df1['Cleanin4'].apply(lambda x: POS_french_token(x))\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    #Get a list from dict with all the verbs\n",
        "\n",
        "    def get_dict_value_verb(POS_dict):\n",
        "          \n",
        "        liste_verb =[]\n",
        "\n",
        "        for key, value in POS_dict.items():\n",
        "          \n",
        "          if value == 'VERB':\n",
        "             liste_verb.append(key)\n",
        "        return liste_verb\n",
        "\n",
        "\n",
        "    df1['Listes_verb'] = df1['POS_LEFFF2'].apply(lambda x: get_dict_value_verb(x))\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Get a liste from dict with all the nouns\n",
        "\n",
        "\n",
        "    def get_dict_value_noun(POS_dict):\n",
        "          \n",
        "        liste_noun =[]\n",
        "\n",
        "        for key, value in POS_dict.items():\n",
        "          \n",
        "            if value == 'NOUN':\n",
        "               liste_noun.append(key)\n",
        "        return liste_noun\n",
        "\n",
        "\n",
        "    df1['Listes_noun'] = df1['POS_LEFFF2'].apply(lambda x: get_dict_value_noun(x))\n",
        "\n",
        "\n",
        "    return df1\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9mpTl73bXB"
      },
      "source": [
        "# Final functions that applied the cleaning functions and the lemma and POS functions\n",
        "\n",
        "def cleaning_Leff(df):\n",
        "  cleaning_general(df)\n",
        "  lemma_and_Leff(df)\n",
        "  return df\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE1euVl02LGg"
      },
      "source": [
        "# Transform list of list into one single list + remove duplicate nouns\n",
        "\n",
        "def flat_list_noun(df1):\n",
        "\n",
        "    cleaning_Leff(df1)\n",
        "    liste_noun = df1['Listes_noun'].tolist()\n",
        "    flat_list_noun = list(itertools.chain(*liste_noun))\n",
        "    flat_list_noun = sorted(flat_list_noun,reverse=True)\n",
        "    flat_list_noun = list(OrderedDict.fromkeys(flat_list_noun))\n",
        "\n",
        "    return flat_list_noun\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def flat_list_verb(df1):\n",
        "\n",
        "    cleaning_Leff(df1)\n",
        "    liste_verb = df1['Listes_verb'].tolist()\n",
        "    flat_list_verb = list(itertools.chain(*liste_verb))\n",
        "    flat_list_verb = sorted(flat_list_verb,reverse=True)\n",
        "    flat_list_verb = list(OrderedDict.fromkeys(flat_list_verb))\n",
        "\n",
        "    return flat_list_verb\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31i21uOe6ryc"
      },
      "source": [
        "## DEAL WITH DECEPTIVE COGNOMES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyf-wv8Hfm53"
      },
      "source": [
        "# Similarity check\n",
        "\n",
        "import difflib\n",
        "\n",
        "def stem_checker(stem1,stem2):\n",
        "\n",
        "  sequence = round(difflib.SequenceMatcher(None,stem1,stem2).ratio()*100)\n",
        "  return sequence \n",
        "\n",
        "# https://www.kite.com/python/docs/difflib.SequenceMatcher\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfpuVsZOfQ9f"
      },
      "source": [
        "# Dataframe with Deceptive cognomes (false friends) \n",
        "\n",
        "main_Dcognomes= pd.read_csv('https://raw.githubusercontent.com/mbayle98/S2_project/main/BIGSCALE/Cognomes_df.txt', delimiter = \"\\t\")\n",
        "\n",
        "main_Dcognomes['eng_stem'] = main_Dcognomes['English'].apply(lambda x: stemmer.stem(x))\n",
        "main_Dcognomes['fr_stem'] = main_Dcognomes['Français'].apply(lambda x: stemmerfr.stem(x))\n",
        "main_Dcognomes['ratio_similarity'] = main_Dcognomes.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "main_Dcognomes_dict = dict(zip(main_Dcognomes.Français,main_Dcognomes.ratio_similarity))\n",
        "main_Dcognomes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQ8nmC17MYa"
      },
      "source": [
        "## DEAL WITH COGNOMES FINAL DATAFRAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7xsyGHkfaNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "0c2f7967-46c7-4a5d-9b85-b0399b4cd3f9"
      },
      "source": [
        "main_df= pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/MAINDF_.txt', delimiter = \"\\t\")\n",
        "main_df = main_df.drop_duplicates()\n",
        "main_dict = dict(zip(main_df.FR,main_df.ratio_similarity))\n",
        "main_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ENG</th>\n",
              "      <th>FR</th>\n",
              "      <th>eng_stem</th>\n",
              "      <th>fr_stem</th>\n",
              "      <th>ratio_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aback</td>\n",
              "      <td>décontenancé</td>\n",
              "      <td>aback</td>\n",
              "      <td>décontenanc</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>airlock</td>\n",
              "      <td>sas</td>\n",
              "      <td>airlock</td>\n",
              "      <td>sas</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>airsick</td>\n",
              "      <td>le mal de l'air</td>\n",
              "      <td>airsick</td>\n",
              "      <td>le mal de l'air</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alack</td>\n",
              "      <td>un manque</td>\n",
              "      <td>alack</td>\n",
              "      <td>un manqu</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15606</th>\n",
              "      <td>vocation</td>\n",
              "      <td>vocation</td>\n",
              "      <td>vocat</td>\n",
              "      <td>vocat</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15607</th>\n",
              "      <td>volition</td>\n",
              "      <td>volition</td>\n",
              "      <td>volit</td>\n",
              "      <td>volit</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15608</th>\n",
              "      <td>westernisation</td>\n",
              "      <td>occidentalisation</td>\n",
              "      <td>westernis</td>\n",
              "      <td>occidentalis</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15609</th>\n",
              "      <td>workstation</td>\n",
              "      <td>poste de travail</td>\n",
              "      <td>workstat</td>\n",
              "      <td>poste de travail</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15610</th>\n",
              "      <td>zonation</td>\n",
              "      <td>zonage</td>\n",
              "      <td>zonat</td>\n",
              "      <td>zonag</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14209 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  ENG                 FR  ...           fr_stem ratio_similarity\n",
              "0               aback       décontenancé  ...       décontenanc               25\n",
              "1             airlock                sas  ...               sas               20\n",
              "2             airsick    le mal de l'air  ...   le mal de l'air               27\n",
              "3               alack          un manque  ...          un manqu               15\n",
              "4                amok               amok  ...              amok              100\n",
              "...               ...                ...  ...               ...              ...\n",
              "15606        vocation           vocation  ...             vocat              100\n",
              "15607        volition           volition  ...             volit              100\n",
              "15608  westernisation  occidentalisation  ...      occidentalis               38\n",
              "15609     workstation   poste de travail  ...  poste de travail               33\n",
              "15610        zonation             zonage  ...             zonag               80\n",
              "\n",
              "[14209 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYVgHRZ5AIH6"
      },
      "source": [
        "def lemma_french_token(text):\n",
        "  doc = nlp(text)\n",
        "  liste1 = []\n",
        "  for token in doc:\n",
        "    liste1.append(token.lemma_)\n",
        "  return liste1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf11Nw_heSYP"
      },
      "source": [
        "def Check_Dcognomes(sentences):\n",
        "  \n",
        "    sentence_clean = \"\"\n",
        "    lemm_sentence = lemma_french_token(sentences)\n",
        "    doc = TreebankWordDetokenizer().detokenize(lemm_sentence)\n",
        "    doc = nlp(doc)\n",
        "\n",
        "    for token in doc:\n",
        "      if token.pos_ == 'NOUN'or token.pos_ == 'VERB' or token.pos_ =='ADJ':\n",
        "        sentence_clean = sentence_clean + ' ' + token.text\n",
        "      \n",
        "    nb_Dcognomes = 0\n",
        "    sentence = sentence_clean.split()\n",
        "    for i in sentence:\n",
        "      if i in main_Dcognomes_dict:\n",
        "        nb_Dcognomes = nb_Dcognomes + 1\n",
        "    \n",
        "    \n",
        "    return nb_Dcognomes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqSvr2D5ekFl"
      },
      "source": [
        "def count_DeceptiveCognome_and_similarity_score(sentences):\n",
        "\n",
        "  sentence_clean = \"\"\n",
        "  lemm_sentence = lemma_french_token(sentences)\n",
        "  doc = TreebankWordDetokenizer().detokenize(lemm_sentence)\n",
        "  doc = nlp(doc)\n",
        "\n",
        "  for token in doc:\n",
        "    if token.pos_ == 'NOUN'or token.pos_ == 'VERB' or token.pos_ =='ADJ':\n",
        "      sentence_clean = sentence_clean + ' ' + token.text\n",
        "\n",
        "  similarity_score_Dcognomes = 0\n",
        "  similarity_score_tt = 0\n",
        "  list_words = []\n",
        "  doc = sentence_clean.split()\n",
        "\n",
        "  for i in doc:\n",
        "    if i in main_Dcognomes_dict.keys():\n",
        "      similarity_score_Dcognomes = similarity_score_Dcognomes + main_Dcognomes_dict[i]\n",
        "      \n",
        "    if i in main_dict.keys():\n",
        "      similarity_score_tt = similarity_score_tt + main_dict[i]\n",
        "\n",
        "    final_score = similarity_score_tt - similarity_score_Dcognomes\n",
        "\n",
        "    return  final_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va1Jt7C6-Mok"
      },
      "source": [
        "def Dcognomes_similarityscore(df1):\n",
        "  \n",
        "    df1['Nb_Dcognomes'] = df1['lemma'].apply(lambda x: Check_Dcognomes(x))\n",
        "    df1['score'] = df1['lemma'].apply(lambda x: count_DeceptiveCognome_and_similarity_score(x))\n",
        "\n",
        "    return df1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "cvWWBubC-gWz",
        "outputId": "725bc3b8-4b56-4a31-f2e6-da5b372d3985"
      },
      "source": [
        "Dcognomes_similarityscore(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Level</th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Cleaning1</th>\n",
              "      <th>Cleaning2</th>\n",
              "      <th>Cleaning3</th>\n",
              "      <th>Cleaning4</th>\n",
              "      <th>lemma</th>\n",
              "      <th>Cleaning7</th>\n",
              "      <th>Cleaning8</th>\n",
              "      <th>POS_LEFFF2</th>\n",
              "      <th>Listes_verb</th>\n",
              "      <th>Listes_noun</th>\n",
              "      <th>Nb_Dcognomes</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B2</td>\n",
              "      <td>la formation s'achèvera par un contrôle pour s...</td>\n",
              "      <td>la formation se achèvera par un contrôle pour ...</td>\n",
              "      <td>la formation se achèvera par un contrôle pour ...</td>\n",
              "      <td>la formation se achèvera par un contrôle pour ...</td>\n",
              "      <td>formation achèvera par contrôle pour savoir pr...</td>\n",
              "      <td>formation achever par contrôle pour savoir pro...</td>\n",
              "      <td>formation achever contrôle propriétaire intégr...</td>\n",
              "      <td>formation achever contrôle propriétaire intégr...</td>\n",
              "      <td>{'formation': 'NOUN', 'achever': 'VERB', 'par'...</td>\n",
              "      <td>[achever, savoir]</td>\n",
              "      <td>[formation, contrôle, propriétaire, connaissance]</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C1</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives des ordonnances ...</td>\n",
              "      <td>un sanction administratif de ordonnance pénal ...</td>\n",
              "      <td>sanction administratif ordonnance pénal possib...</td>\n",
              "      <td>sanction administratif ordonnance pénal détent...</td>\n",
              "      <td>{'un': 'DET', 'sanction': 'NOUN', 'administrat...</td>\n",
              "      <td>[renoncer, suivre, achever]</td>\n",
              "      <td>[sanction, ordonnance, détenteur, chien, forma...</td>\n",
              "      <td>0</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C1</td>\n",
              "      <td>le canidé, d'un âge avancé, a été bien malgré ...</td>\n",
              "      <td>le canidé, de un âge avancé, a été bien malgré...</td>\n",
              "      <td>le canidé de un âge avancé a été bien malgré l...</td>\n",
              "      <td>le canidé de un âge avancé a été bien malgré l...</td>\n",
              "      <td>canidé âge avancé été bien malgré lui coeur co...</td>\n",
              "      <td>canidé âg avancé être bien malgré lui coeur co...</td>\n",
              "      <td>canidé âg avancé malgré coeur commentaire déso...</td>\n",
              "      <td>canidé âg avancé coeur commentaire désobligean...</td>\n",
              "      <td>{'canidé': 'NOUN', 'âg': 'AUX', 'avancé': 'VER...</td>\n",
              "      <td>[avancé, désobligeant, proférer, télévision]</td>\n",
              "      <td>[canidé, coeur, plateau, chaîne]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A2</td>\n",
              "      <td>tout savoir avant d'adopter un chat.</td>\n",
              "      <td>tout savoir avant de adopter un chat.</td>\n",
              "      <td>tout savoir avant de adopter un chat</td>\n",
              "      <td>tout savoir avant de adopter un chat</td>\n",
              "      <td>tout savoir avant adopter chat</td>\n",
              "      <td>tout savoir avant adopter chat</td>\n",
              "      <td>adopter chat</td>\n",
              "      <td>adopter chat</td>\n",
              "      <td>{'tout': 'PRON', 'savoir': 'VERB', 'avant': 'A...</td>\n",
              "      <td>[savoir, adopter]</td>\n",
              "      <td>[chat]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2</td>\n",
              "      <td>le chat, le nouveau « meilleur ami de l'homme ».</td>\n",
              "      <td>le chat, le nouveau « meilleur ami de le homme ».</td>\n",
              "      <td>le chat le nouveau « meilleur ami de le homme »</td>\n",
              "      <td>le chat le nouveau « meilleur ami de le homme »</td>\n",
              "      <td>chat nouveau meilleur ami homme</td>\n",
              "      <td>chat nouveau meilleur ami homme</td>\n",
              "      <td>chat meilleur ami</td>\n",
              "      <td>chat meilleur ami</td>\n",
              "      <td>{'chat': 'NOUN', 'nouveau': 'ADJ', 'meilleur':...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[chat, ami, homme]</td>\n",
              "      <td>0</td>\n",
              "      <td>86.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>C2</td>\n",
              "      <td>il savait que le souvenir même du piano faussa...</td>\n",
              "      <td>il savait que le souvenir même du piano faussa...</td>\n",
              "      <td>il savait que le souvenir même du piano faussa...</td>\n",
              "      <td>il savait que le souvenir même du piano faussa...</td>\n",
              "      <td>savait que souvenir même piano faussait encore...</td>\n",
              "      <td>savoir que souvenir même piano fausser encore ...</td>\n",
              "      <td>souvenir piano fausser plan chose musiqu champ...</td>\n",
              "      <td>souvenir piano fausser plan chose musiqu champ...</td>\n",
              "      <td>{'savoir': 'VERB', 'que': 'PRON', 'souvenir': ...</td>\n",
              "      <td>[savoir, souvenir, fausser, voir, musiqu, sépa...</td>\n",
              "      <td>[piano, plan, chose, champ, clavier, note, tén...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>C2</td>\n",
              "      <td>je n'avais pas de plus grand désir que de voir...</td>\n",
              "      <td>je ne avais pas de plus grand désir que de voi...</td>\n",
              "      <td>je ne avais pas de plus grand désir que de voi...</td>\n",
              "      <td>je ne avais pas de plus grand désir que de voi...</td>\n",
              "      <td>avais pas plus grand désir que voir une tempêt...</td>\n",
              "      <td>avoir pas plus grand désir que voir un tempête...</td>\n",
              "      <td>désir tempête mer beau spectacle moment dévoil...</td>\n",
              "      <td>désir tempête mer beau spectacle moment dévoil...</td>\n",
              "      <td>{'avoir': 'VERB', 'pas': 'ADV', 'plus': 'ADV',...</td>\n",
              "      <td>[avoir, voir, dévoiler, savoir, combiner, conn...</td>\n",
              "      <td>[désir, tempête, mer, spectacle, moment, vie, ...</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>C2</td>\n",
              "      <td>car souvent dans l'une on trouve égaré un jour...</td>\n",
              "      <td>car souvent dans le une on trouve égaré un jou...</td>\n",
              "      <td>car souvent dans le une on trouve égaré un jou...</td>\n",
              "      <td>car souvent dans le une on trouve égaré un jou...</td>\n",
              "      <td>car souvent dans une trouve égaré jour une aut...</td>\n",
              "      <td>car souvent dans un trouve égarer jour un autr...</td>\n",
              "      <td>car souvent trouve égarer vivre évoque aussitô...</td>\n",
              "      <td>trouve égarer vivre évoque aussitôt désirer pl...</td>\n",
              "      <td>{'car': 'CCONJ', 'souvent': 'ADV', 'dans': 'AD...</td>\n",
              "      <td>[trouve, égarer, vivre, évoque, désirer, inter...</td>\n",
              "      <td>[jour, plaisir, rêve, tour, chapitre, calendri...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>C2</td>\n",
              "      <td>un souffle de vent les dispersait, la pierre s...</td>\n",
              "      <td>un souffle de vent les dispersait, la pierre s...</td>\n",
              "      <td>un souffle de vent les dispersait la pierre se...</td>\n",
              "      <td>un souffle de vent les dispersait la pierre se...</td>\n",
              "      <td>souffle vent les dispersait pierre était nouve...</td>\n",
              "      <td>souffle ver le disperser pierre être nouveau a...</td>\n",
              "      <td>souffle ver disperser pierre assombrie apprivo...</td>\n",
              "      <td>souffle ver disperser pierre assombrie apprivo...</td>\n",
              "      <td>{'souffle': 'ADJ', 'ver': 'ADJ', 'le': 'DET', ...</td>\n",
              "      <td>[disperser, apprivoisé, revenir, recommencer, ...</td>\n",
              "      <td>[pierre, assombrie, crescendo, fin, ouverture,...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>C2</td>\n",
              "      <td>plus tard, il arrive que, devenus habiles dans...</td>\n",
              "      <td>plus tard, il arrive que, devenus habiles dans...</td>\n",
              "      <td>plus tard il arrive que devenus habiles dans l...</td>\n",
              "      <td>plus tard il arrive que devenus habiles dans l...</td>\n",
              "      <td>plus tard arrive que devenus habiles dans cult...</td>\n",
              "      <td>plus tard arrive que devenu habile dans cultur...</td>\n",
              "      <td>tard arrive devenu habile culture plaisir cont...</td>\n",
              "      <td>tard arrive devenu habile culture plaisir cont...</td>\n",
              "      <td>{'plus': 'ADV', 'tard': 'ADV', 'arrive': 'VERB...</td>\n",
              "      <td>[arrive, devenu, avoir, penser, inquiet, savoi...</td>\n",
              "      <td>[culture, plaisir, contention, femme, pensai, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1030 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Level  ...  score\n",
              "0       B2  ...  100.0\n",
              "1       C1  ...  100.0\n",
              "2       C1  ...    0.0\n",
              "3       A2  ...    0.0\n",
              "4       A2  ...   86.0\n",
              "...    ...  ...    ...\n",
              "1025    C2  ...    0.0\n",
              "1026    C2  ...    0.0\n",
              "1027    C2  ...    0.0\n",
              "1028    C2  ...    0.0\n",
              "1029    C2  ...    0.0\n",
              "\n",
              "[1030 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "q5vw5sNgBA-x",
        "outputId": "3ed4a925-de41-4481-e060-fbf3c8b7d24e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df.groupby('Level')['Nb_Dcognomes'].mean().plot.bar(color = 'black')\n",
        "\n",
        "plt.title('Average by level of the number of deceptive cognates present in the sentences ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Average by level of the number of deceptive cognates present in the sentences ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hNldGLWC6MX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
