{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Functions_with_initial_datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1iT6SmoGo9Sd",
        "UoqIGtABJYwc",
        "EjQpP2RJCW9V"
      ],
      "authorship_tag": "ABX9TyOdxLI4wmSRpmBQeUEvjZ27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epicalekspwner/BigScaleAnalytics2021/blob/main/Functions_with_initial_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoqIGtABJYwc"
      },
      "source": [
        "### Libraries and lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wz0bXwZBdky"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rr9GaNqtBN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baab95e5-6340-4815-ae4e-bb6c41e4d6e2"
      },
      "source": [
        "pip install spacy-lefff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-lefff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/21/a326ed06c4175ec07baf61ee563dbc5bc06637e5d863f11ce9409d007203/spacy-lefff-0.4.0.tar.gz (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 6.0MB/s \n",
            "\u001b[?25hCollecting spacy<3.0.5,>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/c4/a5a8aa936e1b7fbba1780863447a51684dabb7f8855931f2510d4637d641/spacy-3.0.4-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 310kB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (20.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.19.5)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.23.0)\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.5MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.0.5)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.7.4.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (54.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.8.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (4.41.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.11.3)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.4)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.1.1)\n",
            "Building wheels for collected packages: spacy-lefff, smart-open\n",
            "  Building wheel for spacy-lefff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lefff: filename=spacy_lefff-0.4.0-cp37-none-any.whl size=2929893 sha256=f2f0a6e16e6875795a1f52b72884756dba0c9fe51322d3852480193540a83ef9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/1c/f0/9b95e4e74005afbfe54aa126484febacf0fd27feffa3e9ad45\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=c5eca0cf3a3f58d68537c1e39b1ea351ccb69d25b8754b22b2e03b6854a6b237\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built spacy-lefff smart-open\n",
            "Installing collected packages: catalogue, srsly, typer, pydantic, thinc, smart-open, pathy, spacy-legacy, spacy, spacy-lefff\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.4 spacy-lefff-0.4.0 spacy-legacy-3.0.2 srsly-2.4.1 thinc-8.0.2 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak_u5r4kBnwS",
        "outputId": "7aa6a26a-cd56-4e3e-c98b-26bd0a4d7bd5"
      },
      "source": [
        "# Spacy library\n",
        "\n",
        "!python -m spacy download fr_core_news_md\n",
        "\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import spacy\n",
        "from spacy_lefff import LefffLemmatizer\n",
        "from spacy.language import Language\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_md\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-12 19:52:21.465994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting fr-core-news-md==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.0.0/fr_core_news_md-3.0.0-py3-none-any.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 97kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (7.1.2)\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LJL0FB9Sog5"
      },
      "source": [
        "# NLTK --> Stemming\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmerfr = SnowballStemmer(\"french\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppj2_EpVSvmF",
        "outputId": "9c96e341-4732-4355-f281-35abe70c21f2"
      },
      "source": [
        "pip install deep-translator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b3/53b50057bf1a5ebbb8a77c19c42ae721129ebe1b8478029f2121a5914f07/deep_translator-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (1.24.3)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNfQH0hMTqw2"
      },
      "source": [
        "from deep_translator import GoogleTranslator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bkO2Uy2Ee8C"
      },
      "source": [
        "# Special lists/dict of char\n",
        "\n",
        "liste_special_letters = ['é','è','ê','ë','à','ä','â','ï','î','ö','ô','ü','ù','û','ç']\n",
        "contractions_dict = { \"d'\": \"de \",\"n'\":\"ne \",\"l'\":\"le \",\"s'\":\"se \",\"c'\":\"ce \",\"j'\":\"je \",\"t'\":\"tu \",\"qu'\":\"que \"}\n",
        "stopWord = ['le','la','les','de','des','se','ce','à']\n",
        "Fr_stp_modif = {'a', 'abord', 'afin', 'ah', 'ai', 'aie', 'ainsi',\n",
        " 'alors', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', \n",
        " 'assez',  'au', 'aucun', 'aucune', 'aujourd', \"aujourd'hui\", 'aupres',\n",
        " 'auquel', 'aussi', 'autre', 'autrement', 'autres', 'autrui',\n",
        " 'aux',  'auxquelles', 'auxquels',  'avant', 'avec',  'bas', 'basee', 'car', 'ce', 'ceci',\n",
        " 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', 'celui',\n",
        " 'celui-ci', 'celui-là', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', 'cet', 'cette', 'ceux',\n",
        " 'ceux ci', 'ceux là', 'chacun', 'chacune', 'chaque', 'chez', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième',\n",
        " 'combien', 'comme', 'comment', 'compris', 'concernant', 'da', 'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis',\n",
        " 'derriere', 'derrière', 'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant',\n",
        " 'devers', 'devra', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', 'directe',\n",
        " 'directement', 'divers', 'diverse', 'diverses', 'dix', 'dix huit', 'dix neuf', 'dix sept', 'dixième', 'donc', 'dont', 'douze',\n",
        " 'douzième', 'du', 'duquel', 'durant','dès', 'désormais', 'effet','egale', 'egalement', 'egales', 'eh', 'elle', 'elle même',\n",
        " 'elles', 'elles mêmes', 'en', 'encore', 'enfin', 'entre','envers', 'environ','et', 'etc', 'eux', 'eux mêmes',\n",
        " 'exactement',\n",
        " 'excepté',\n",
        "  'chat'\n",
        "  'chien'\n",
        " 'façon',\n",
        "\n",
        " 'gens',\n",
        " 'ha',\n",
        " 'hem',\n",
        " 'hep',\n",
        " 'hi',\n",
        " 'ho',\n",
        " 'hormis',\n",
        " 'hors',\n",
        " 'hou',\n",
        " 'houp',\n",
        " 'hue',\n",
        " 'hui',\n",
        " 'huit',\n",
        " 'huitième',\n",
        " 'hé',\n",
        " 'i',\n",
        " 'il',\n",
        " 'ils',\n",
        " 'importe',\n",
        " \"j \",\n",
        " 'je',\n",
        " 'jusqu',\n",
        " 'jusque',\n",
        " 'juste',\n",
        " 'j ',\n",
        " \"l \",\n",
        " 'la',\n",
        " 'laquelle',\n",
        " 'le',\n",
        " 'lequel',\n",
        " 'les',\n",
        " 'lesquelles',\n",
        " 'lesquels',\n",
        " 'leur',\n",
        " 'leurs',\n",
        " 'longtemps',\n",
        " 'lors',\n",
        " 'lorsque',\n",
        " 'lui',\n",
        " 'lui meme',\n",
        " 'lui même',\n",
        " 'là',\n",
        " 'lès',\n",
        " 'l ',\n",
        " \"m \",\n",
        " 'ma',\n",
        " 'maint',\n",
        " 'maintenant',\n",
        " 'mais',\n",
        " 'malgré',\n",
        " 'me',\n",
        " 'meme',\n",
        " 'memes',\n",
        " 'merci',\n",
        " 'mes',\n",
        " 'mien',\n",
        " 'mienne',\n",
        " 'miennes',\n",
        " 'miens',\n",
        " 'mille',\n",
        " 'moi',\n",
        " 'moi meme',\n",
        " 'moi même',\n",
        " 'moindres',\n",
        " 'moins',\n",
        " 'mon',\n",
        " 'même',\n",
        " 'mêmes',\n",
        " 'm ',\n",
        " \"n \",\n",
        " 'na',\n",
        " 'ne',\n",
        " 'neanmoins',\n",
        " 'neuvième',\n",
        " 'ni',\n",
        " 'nombreuses',\n",
        " 'nombreux',\n",
        " 'nos',\n",
        " 'notamment',\n",
        " 'notre',\n",
        " 'nous',\n",
        " 'nous-mêmes',\n",
        " 'nouvea',\n",
        " 'nul',\n",
        " 'néanmoins',\n",
        " 'nôtre',\n",
        " 'nôtres',\n",
        " 'n’',\n",
        " 'o',\n",
        " 'on',\n",
        " \n",
        " 'onze',\n",
        " 'onzième',\n",
        " 'ore',\n",
        " 'ou',\n",
        " 'ouias',\n",
        " 'oust',\n",
        " 'outre',\n",
        "\n",
        " 'ouverte',\n",
        " 'ouverts',\n",
        " 'où',\n",
        " 'par',\n",
        " 'parce',\n",
        " 'parfois',\n",
        " 'parmi',\n",
        " 'parseme',\n",
        " 'partant',\n",
        " 'pas',\n",
        " 'pendant',\n",
        " 'pense',\n",
        " 'permet',\n",
        " 'personne',\n",
        " 'peu',\n",
        "\n",
        " 'plus',\n",
        " 'plusieurs',\n",
        " 'plutôt',\n",
        " 'possible',\n",
        " 'possibles',\n",
        " 'pour',\n",
        " 'pourquoi',\n",
        "\n",
        " 'prealable',\n",
        " 'precisement',\n",
        " 'premier',\n",
        " 'première',\n",
        " 'premièrement',\n",
        " 'pres',\n",
        " 'procedant',\n",
        " 'proche',\n",
        " 'près',\n",
        " 'pu',\n",
        " 'puis',\n",
        " 'puisque',\n",
        " \"qu'\",\n",
        " 'quand',\n",
        " 'quant',\n",
        " 'quant à soi',\n",
        " 'quanta',\n",
        " 'quarante',\n",
        " 'quatorze',\n",
        " 'quatre',\n",
        " 'quatre vingt',\n",
        " 'quatrième',\n",
        " 'quatrièmement',\n",
        " 'que',\n",
        " 'quel',\n",
        " 'quelconque',\n",
        " 'quelle',\n",
        " 'quelles',\n",
        " \"quelqu'un\",\n",
        " 'quelque',\n",
        " 'quelques',\n",
        " 'quels',\n",
        " 'qui',\n",
        " 'quiconque',\n",
        " 'quinze',\n",
        " 'quoi',\n",
        " 'quoique',\n",
        " 'qu’',\n",
        " 'relative',\n",
        " 'relativement',\n",
        " \n",
        "\n",
        " 'retour',\n",
        " 'revoici',\n",
        " 'revoilà',\n",
        " \"s \",\n",
        " 'sa',\n",
        " 'sait',\n",
        " 'sans',\n",
        " 'sauf',\n",
        " 'se',\n",
        " 'seize',\n",
        " 'selon',\n",
        " 'semblable',\n",
        " 'semblaient',\n",
        " 'semble',\n",
        " 'semblent',\n",
        " 'sent',\n",
        " 'sept',\n",
        " 'septième',\n",
        "\n",
        " 'ses',\n",
        " 'seul',\n",
        " 'seule',\n",
        " 'seulement',\n",
        " 'si',\n",
        " 'sien',\n",
        " 'sienne',\n",
        " 'siennes',\n",
        " 'siens',\n",
        " 'sinon',\n",
        " 'six',\n",
        " 'sixième',\n",
        " 'soi',\n",
        " 'soi même',\n",
        " 'soit',\n",
        " 'soixante',\n",
        " 'son',\n",
        "\n",
        " 'sous',\n",
        " 'souvent',\n",
        " 'specifique',\n",
        " 'specifiques',\n",
        " 'stop',\n",
        " 'suffisant',\n",
        " 'suffisante',\n",
        "\n",
        " 'suivant',\n",
        " 'suivante',\n",
        " 'suivantes',\n",
        " 'suivants',\n",
        " 'suivre',\n",
        " 'sur',\n",
        " 'surtout',\n",
        " 's ',\n",
        " \"t \",\n",
        " 'ta',\n",
        " 'tant',\n",
        " 'te',\n",
        " 'tel',\n",
        " 'telle',\n",
        " 'tellement',\n",
        " 'telles',\n",
        " 'tels',\n",
        " 'tenant',\n",
        "\n",
        " 'tente',\n",
        " 'tes',\n",
        " 'tien',\n",
        " 'tienne',\n",
        " 'tiennes',\n",
        " 'tiens',\n",
        " 'toi',\n",
        " 'toi-même',\n",
        " 'ton',\n",
        " 'touchant',\n",
        " 'toujours',\n",
        " 'tous',\n",
        " 'tout',\n",
        " 'toute',\n",
        " 'toutes',\n",
        " 'treize',\n",
        " 'trente',\n",
        " 'tres',\n",
        " 'trois',\n",
        " 'troisième',\n",
        " 'troisièmement',\n",
        " 'tu',\n",
        " 'té',\n",
        " 't ',\n",
        " 'un',\n",
        " 'une',\n",
        " 'unes',\n",
        " 'uns',\n",
        " 'va',\n",
        " 'vais',\n",
        " 'vas',\n",
        " 'vers',\n",
        " 'via',\n",
        " 'vingt',\n",
        " 'voici',\n",
        " 'voilà',\n",
        " 'vont',\n",
        " 'vos',\n",
        " 'votre',\n",
        " 'vous',\n",
        " 'vous mêmes',\n",
        " 'vu',\n",
        " 'vé',\n",
        " 'vôtre',\n",
        " 'vôtres',\n",
        " 'à',\n",
        " 'â',\n",
        " 'ça',\n",
        " 'ès',\n",
        " 'ô'}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b9LBUAlpDQr"
      },
      "source": [
        "## 2/ Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S2fxHFSJhFl"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS2VTNEcJGMV"
      },
      "source": [
        "# Dataframe import\n",
        "\n",
        "Phrases = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Phrasee.txt', delimiter = \"\\t\")\n",
        "Niveau = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Niveau.txt', delimiter = \"\\t\" )\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['Level'] = Niveau['Type (Maxime)']\n",
        "df['Sentences'] = Phrases['Phrases']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "L4aXE6Uck-DM",
        "outputId": "b42ad95e-aa45-41f1-8075-a658256a5024"
      },
      "source": [
        "# Outliers?\n",
        "\n",
        "df['Level'].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6761d6ad10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXN3LGElicsw"
      },
      "source": [
        "# Data cleaning 1 --> char replacement + lower\n",
        "\n",
        "df['Sentences'] = df['Sentences'].apply(lambda x: x.replace(\"’\",\"'\"))\n",
        "df['Sentences'] = df['Sentences'].apply(lambda x: x.lower())\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Data cleaning 2 --> Contractions expanding\n",
        "\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "    # Function for expanding contractions\n",
        "\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)\n",
        "\n",
        "df['Cleaning1'] = df['Sentences'].apply(lambda x: expand_contractions(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Data cleaning 3 --> Punctuation removal\n",
        "\n",
        "def punctuation_removal(text):\n",
        "    all_list = [char for char in text if char not in string.punctuation]\n",
        "    clean_str = ''.join(all_list)\n",
        "    return clean_str\n",
        "\n",
        "df['Cleaning2'] = df['Cleaning1'].apply(lambda x: punctuation_removal(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Data cleaning 4 --> Remove special char (took into account french special char e.g. \"ç\",\"é\")\n",
        "\n",
        "def bin_spe(tweet):\n",
        "    tweet = ' '.join(re.sub(\"[^0-9a-zÀ-ÿ-A-Z-ç \\t]\",\" \", tweet).split())\n",
        "    return tweet\n",
        "\n",
        "df['Cleaning3'] = df['Cleaning2'].apply(lambda x: bin_spe(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Data cleaning 5 --> Stop words removal\n",
        "\n",
        "def stopword_removal(text):\n",
        "    all_list = [char for char in text if char not in stopWord]\n",
        "    clean_str = ''.join(all_list)\n",
        "    return clean_str\n",
        "\n",
        "df['Cleaning3'] = df['Cleaning2'].apply(lambda x: stopword_removal(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Data cleaning 6 --> remove alone letter and words with len() ==2\n",
        "\n",
        "def remove_alone_letters_and_len2(texte):\n",
        "    texte = ' '.join(i for i in texte.split() if not len(i) == 1 and not len(i) == 2)\n",
        "    return texte\n",
        "\n",
        "df['Cleaning4'] = df['Cleaning3'].apply(lambda x: remove_alone_letters_and_len2(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY6L3eEzv0ID"
      },
      "source": [
        "## 3/ Leff and lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCGHGCzAv5Il"
      },
      "source": [
        "df1 = pd.DataFrame()\n",
        "df1['sentences_cleaned'] = df['Cleaning4']\n",
        "df1['Level'] = df['Level']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbLMCRRI3_Cl"
      },
      "source": [
        "# Count top 100 words whole dataset\n",
        "\n",
        "Counter(\" \".join(df1[\"sentences_cleaned\"]).split()).most_common(100)\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Lemmatization --> OUTPUT: Token\n",
        "\n",
        "def lemma_french_token(text):\n",
        "  doc = nlp(text)\n",
        "  liste1 = []\n",
        "  for token in doc:\n",
        "     liste1.append(token.lemma_)\n",
        "  return liste1\n",
        "\n",
        "df1['lemma'] = df1['sentences_cleaned'].apply(lambda x: lemma_french_token(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Lemmatization --> OUTPUT: sentence\n",
        "\n",
        "def lemma_french_full_sentence(text):\n",
        "  doc = nlp(text)\n",
        "  lemmatized_output = ' '.join([w.lemma_ for w in doc])\n",
        "  return lemmatized_output\n",
        "\n",
        "df1['lemma'] = df1['sentences_cleaned'].apply(lambda x: lemma_french_full_sentence(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Count to 100 words after lemma whole dataset\n",
        "\n",
        "Top_100_lemma = Counter(\" \".join(df1[\"lemma\"]).split()).most_common(100)\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Get a list of the top 100 words in the dataset\n",
        "\n",
        "def count_to_list(my_list_count_100):\n",
        "  list2 = []\n",
        "  for my_tuple in my_list_count_100:\n",
        "      list2.append(my_tuple[0])\n",
        "  return list2\n",
        "\n",
        "List_top_100 = count_to_list(Top_100_lemma)\n",
        "\n",
        "\n",
        "List_top_100\n",
        "\n",
        "# Data cleaning 7 --> Common word removal\n",
        "\n",
        "def common_word_removal(text):\n",
        "\n",
        "    all_list =  \" \".join([word for word in text.split() if word not in List_top_100])\n",
        "    \n",
        "    return all_list\n",
        "\n",
        "df1['Cleaning7'] = df1['lemma'].apply(lambda x: common_word_removal(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def List_STP_word_removal(text):\n",
        "\n",
        "    all_list =  \" \".join([word for word in text.split() if word not in Fr_stp_modif])\n",
        "    \n",
        "    return all_list\n",
        "\n",
        "df1['Cleaning8'] = df1['Cleaning7'].apply(lambda x: List_STP_word_removal(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57VB65XYtbUa"
      },
      "source": [
        "# POS_LEFFF\n",
        "\n",
        "def POS_french_token(sentences):\n",
        "    dict1 = {}\n",
        "    doc = nlp(sentences)\n",
        "    for token in doc:\n",
        "      dict1.update({token.text : token.pos_})\n",
        "    return dict1\n",
        "\n",
        "# df1['POS_LEFFF'] = df1['Cleaning8'].apply(lambda x: POS_french_token(x))\n",
        "\n",
        "df1['POS_LEFFF2'] = df1['lemma'].apply(lambda x: POS_french_token(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGCfuV94wey3"
      },
      "source": [
        "# Get a list from dict with all the verbs\n",
        "\n",
        "def get_dict_value_verb(POS_dict):\n",
        "    \n",
        "    liste_verb =[]\n",
        "\n",
        "    for key, value in POS_dict.items():\n",
        "     \n",
        "      if value == 'VERB':\n",
        "        liste_verb.append(key)\n",
        "    return liste_verb\n",
        "\n",
        "\n",
        "df1['Listes_verb'] = df1['POS_LEFFF2'].apply(lambda x: get_dict_value_verb(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Get a liste from dict with all the nouns\n",
        "\n",
        "\n",
        "def get_dict_value_noun(POS_dict):\n",
        "    \n",
        "    liste_noun =[]\n",
        "\n",
        "    for key, value in POS_dict.items():\n",
        "     \n",
        "      if value == 'NOUN':\n",
        "        liste_noun.append(key)\n",
        "    return liste_noun\n",
        "\n",
        "\n",
        "df1['Listes_noun'] = df1['POS_LEFFF2'].apply(lambda x: get_dict_value_noun(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwJqcSXGEo-n"
      },
      "source": [
        "# Transform list of list into one single list + remove duplicate nouns\n",
        "\n",
        "\n",
        "liste_noun = df1['Listes_noun'].tolist()\n",
        "flat_list_noun = list(itertools.chain(*liste_noun))\n",
        "flat_list_noun = sorted(flat_list_noun,reverse=True)\n",
        "flat_list_noun = list(OrderedDict.fromkeys(flat_list_noun))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "liste_verb = df1['Listes_verb'].tolist()\n",
        "flat_list_verb = list(itertools.chain(*liste_verb))\n",
        "flat_list_verb = sorted(flat_list_verb,reverse=True)\n",
        "flat_list_verb = list(OrderedDict.fromkeys(flat_list_verb))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EBgN168MJL9"
      },
      "source": [
        "# Similarity check\n",
        "\n",
        "import difflib\n",
        "\n",
        "def stem_checker(stem1,stem2):\n",
        "\n",
        "  sequence = round(difflib.SequenceMatcher(None,stem1,stem2).ratio()*100)\n",
        "  return sequence \n",
        "\n",
        "# https://www.kite.com/python/docs/difflib.SequenceMatcher\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQybiOGCHCd"
      },
      "source": [
        "### Part 1) Translate, suffixes, similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4GzqW5fDSx3"
      },
      "source": [
        "# Get all the nouns that finish with a specific suffixe\n",
        "\n",
        "liste_tion = [w for w in flat_list_noun if w.endswith('tion')]\n",
        "liste_ien = [w for w in flat_list_noun if w.endswith('ien')]\n",
        "liste_ienne = [w for w in flat_list_noun if w.endswith('ienne')]\n",
        "liste_e = [w for w in flat_list_noun if w.endswith('é')]\n",
        "liste_ite = [w for w in flat_list_noun if w.endswith('ite')]\n",
        "liste_isme = [w for w in flat_list_noun if w.endswith('isme')]\n",
        "liste_ment = [w for w in flat_list_noun if w.endswith('ment')]\n",
        "liste_eur = [w for w in flat_list_noun if w.endswith('eur')]\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Get all the verbs that finish with a specific suffixe\n",
        "\n",
        "liste_er = [w for w in flat_list_verb if w.endswith('er')]\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e8hbwSkqQqi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "738a72fe-3b15-40aa-a8f4-66748349b0ba"
      },
      "source": [
        "# English words upload\n",
        "\n",
        "df_eng_words = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Eng_Frenche.csv', delimiter = \"\\t\")\n",
        "df_eng_words['aardvark'] = df_eng_words['aardvark'].apply(lambda x : str(x))\n",
        "Liste_eng_words = df_eng_words.values.tolist()\n",
        "Liste_eng_words = list(itertools.chain(*Liste_eng_words))\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "liste_eng_tion = [w for w in Liste_eng_words if w.endswith('tion')]\n",
        "'''\n",
        "liste_eng_able = [w for w in Liste_eng_words if w.endswith('able')]\n",
        "liste_eng_ate = [w for w in Liste_eng_words if w.endswith('ate')]\n",
        "liste_eng_y = [w for w in Liste_eng_words if w.endswith('y')]\n",
        "\n",
        "liste_eng_ary = [w for w in Liste_eng_words if w.endswith('ary')]\n",
        "liste_eng_or = [w for w in Liste_eng_words if w.endswith('or')]\n",
        "liste_eng_a = [w for w in Liste_eng_words if w.endswith('a')]\n",
        "\n",
        "liste_eng_e = [w for w in Liste_eng_words if w.endswith('e')]\n",
        "liste_eng_ish = [w for w in Liste_eng_words if w.endswith('ish')]\n",
        "liste_eng_c = [w for w in Liste_eng_words if w.endswith('c')]\n",
        "\n",
        "liste_eng_t = [w for w in Liste_eng_words if w.endswith('t')]\n",
        "liste_eng_k = [w for w in Liste_eng_words if w.endswith('k')] '''\n",
        "\n",
        "#-------------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nliste_eng_able = [w for w in Liste_eng_words if w.endswith('able')]\\nliste_eng_ate = [w for w in Liste_eng_words if w.endswith('ate')]\\nliste_eng_y = [w for w in Liste_eng_words if w.endswith('y')]\\n\\nliste_eng_ary = [w for w in Liste_eng_words if w.endswith('ary')]\\nliste_eng_or = [w for w in Liste_eng_words if w.endswith('or')]\\nliste_eng_a = [w for w in Liste_eng_words if w.endswith('a')]\\n\\nliste_eng_e = [w for w in Liste_eng_words if w.endswith('e')]\\nliste_eng_ish = [w for w in Liste_eng_words if w.endswith('ish')]\\nliste_eng_c = [w for w in Liste_eng_words if w.endswith('c')]\\n\\nliste_eng_t = [w for w in Liste_eng_words if w.endswith('t')]\\nliste_eng_k = [w for w in Liste_eng_words if w.endswith('k')] \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB-8sMadHfWq"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "liste_eng_tion = [w for w in Liste_eng_words if w.endswith('tion')]\n",
        "ENG_tion = pd.DataFrame(liste_eng_tion,columns=['ENG_tion'])\n",
        "ENG_tion['FR_tion'] = ENG_tion['ENG_tion'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_tion['eng_stem'] = ENG_tion['ENG_tion'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_tion['fr_stem'] = ENG_tion['FR_tion'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_tion['ratio_similarity'] = ENG_tion.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_tion.to_excel(\"ENG_tion.xls\")\n",
        "files.download('ENG_tion.xls')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vadylbuFpury"
      },
      "source": [
        "'''ENG_able = pd.DataFrame(liste_eng_able,columns=['ENG_able'])\n",
        "ENG_able['FR_able'] = ENG_able['ENG_able'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_able['eng_stem'] = ENG_able['ENG_able'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_able['fr_stem'] = ENG_able['FR_able'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_able['ratio_similarity'] = ENG_able.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "ENG_able.to_excel(\"ENG_able.xls\")\n",
        "files.download('ENG_able.xls')\n",
        "\n",
        "ENG_ate = pd.DataFrame(liste_eng_ate,columns=['ENG_ate'])\n",
        "ENG_ate['FR_ate'] = ENG_ate['ENG_ate'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_ate['eng_stem'] = ENG_ate['ENG_ate'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_ate['fr_stem'] = ENG_ate['FR_ate'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_ate['ratio_similarity'] = ENG_ate.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_ate.to_excel(\"ENG_ate.xls\")\n",
        "files.download('ENG_ate.xls')\n",
        "\n",
        "ENG_y = pd.DataFrame(liste_eng_y,columns=['ENG_y'])\n",
        "ENG_y['FR_y'] = ENG_y['ENG_y'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_y['eng_stem'] = ENG_y['ENG_y'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_y['fr_stem'] = ENG_y['FR_y'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_y['ratio_similarity'] = ENG_y.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_y.to_excel(\"ENG_y.xls\")\n",
        "files.download('ENG_y.xls')\n",
        "\n",
        "ENG_ary = pd.DataFrame(liste_eng_ary,columns=['ENG_ary'])\n",
        "ENG_ary['FR_ary'] = ENG_ary['ENG_ary'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_ary['eng_stem'] = ENG_ary['ENG_ary'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_ary['fr_stem'] = ENG_ary['FR_ary'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_ary['ratio_similarity'] = ENG_ary.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_ary.to_excel(\"ENG_ary.xls\")\n",
        "files.download('ENG_ary.xls')\n",
        "\n",
        "\n",
        "ENG_or = pd.DataFrame(liste_eng_or,columns=['ENG_or'])\n",
        "ENG_or['FR_or'] = ENG_or['ENG_or'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_or['eng_stem'] = ENG_or['ENG_or'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_or['fr_stem'] = ENG_or['FR_or'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_or['ratio_similarity'] = ENG_or.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_or.to_excel(\"ENG_or.xls\")\n",
        "files.download('ENG_or.xls')\n",
        "\n",
        "\n",
        "ENG_a = pd.DataFrame(liste_eng_a,columns=['ENG_a'])\n",
        "ENG_a['FR_a'] = ENG_a['ENG_a'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_a['eng_stem'] = ENG_a['ENG_a'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_a['fr_stem'] = ENG_a['FR_a'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_a['ratio_similarity'] = ENG_a.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_a.to_excel(\"ENG_a.xls\")\n",
        "files.download('ENG_a.xls')\n",
        "\n",
        "\n",
        "ENG_e = pd.DataFrame(liste_eng_e,columns=['ENG_e'])\n",
        "ENG_e['FR_e'] = ENG_e['ENG_e'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_e['eng_stem'] = ENG_e['ENG_e'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_e['fr_stem'] = ENG_e['FR_e'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_e['ratio_similarity'] = ENG_e.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "ENG_e.to_excel(\"ENG_e.xls\")\n",
        "files.download('ENG_e.xls')\n",
        "\n",
        "\n",
        "\n",
        "ENG_ish = pd.DataFrame(liste_eng_ish,columns=['ENG_ish'])\n",
        "ENG_ish['FR_ish'] = ENG_ish['ENG_ish'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_ish['eng_stem'] = ENG_ish['ENG_ish'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_ish['fr_stem'] = ENG_ish['FR_ish'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_ish['ratio_similarity'] = ENG_ish.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "ENG_ish.to_excel(\"ENG_ish.xls\")\n",
        "files.download('ENG_ish.xls')\n",
        "\n",
        "ENG_t = pd.DataFrame(liste_eng_t,columns=['ENG_t'])\n",
        "ENG_t['FR_t'] = ENG_t['ENG_t'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_t['eng_stem'] = ENG_t['ENG_t'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_t['fr_stem'] = ENG_t['FR_t'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_t['ratio_similarity'] = ENG_t.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_t.to_excel(\"ENG_t.xls\")\n",
        "files.download('ENG_t.xls')\n",
        "\n",
        "ENG_k = pd.DataFrame(liste_eng_k,columns=['ENG_k'])\n",
        "ENG_k['FR_k'] = ENG_k['ENG_k'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "ENG_k['eng_stem'] = ENG_k['ENG_k'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_k['fr_stem'] = ENG_k['FR_k'].apply(lambda x: stemmerfr.stem(x))\n",
        "ENG_k['ratio_similarity'] = ENG_k.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "\n",
        "ENG_k.to_excel(\"ENG_k.xls\")\n",
        "files.download('ENG_k.xls')\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqrIy8Cnr-Li"
      },
      "source": [
        "# Translate into english the list of words ending with specific suffixes\n",
        "\n",
        "'''\n",
        "ENG_tion = pd.DataFrame(liste_eng_tion,columns=['ENG_tion'])\n",
        "ENG_tion['FR_tion'] = ENG_tion['ENG_tion'].apply(lambda x: GoogleTranslator(source='en', target='fr').translate(x))\n",
        "\n",
        "ENG_tion['eng_stem'] = ENG_tion['ENG_tion'].apply(lambda x: stemmer.stem(x))\n",
        "ENG_tion['fr_stem'] = ENG_tion['FR_tion'].apply(lambda x: stemmerfr.stem(x))\n",
        "\n",
        "ENG_tion['ratio_similarity'] = ENG_tion.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "df_words_tion_low_similarity = ENG_tion.loc[(ENG_tion['ratio_similarity'] > 0) & (ENG_tion['ratio_similarity'] < 60)]\n",
        "df_words_tion_high_similarity = ENG_tion.loc[ENG_tion['ratio_similarity'] > 70 ]\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "ENG_tion.to_excel(\"eng_fr.xls\")\n",
        "files.download('eng_fr.xls')\n",
        "\n",
        "# Translate into english the list of words ending with specific suffixes\n",
        "\n",
        "tion = pd.DataFrame(liste_tion,columns=['tion_fr'])\n",
        "tion['eng'] = tion['tion_fr'].apply(lambda x: GoogleTranslator(source='fr', target='en').translate(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Apply stemming to both French and English nouns and see if root is the same\n",
        "\n",
        "tion['eng_stem'] = tion['eng'].apply(lambda x: stemmer.stem(x))\n",
        "tion['fr_stem'] = tion['tion_fr'].apply(lambda x: stemmerfr.stem(x))\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjQpP2RJCW9V"
      },
      "source": [
        "### Part 2) Upload df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "wkkfhrlbCeul",
        "outputId": "13a6ce0b-8494-4bbf-90eb-8788bd37de17"
      },
      "source": [
        "main_df= pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/MAINDF_.txt', delimiter = \"\\t\")\n",
        "main_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ENG</th>\n",
              "      <th>FR</th>\n",
              "      <th>eng_stem</th>\n",
              "      <th>fr_stem</th>\n",
              "      <th>ratio_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aback</td>\n",
              "      <td>décontenancé</td>\n",
              "      <td>aback</td>\n",
              "      <td>décontenanc</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>airlock</td>\n",
              "      <td>sas</td>\n",
              "      <td>airlock</td>\n",
              "      <td>sas</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>airsick</td>\n",
              "      <td>le mal de l'air</td>\n",
              "      <td>airsick</td>\n",
              "      <td>le mal de l'air</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alack</td>\n",
              "      <td>un manque</td>\n",
              "      <td>alack</td>\n",
              "      <td>un manqu</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15606</th>\n",
              "      <td>vocation</td>\n",
              "      <td>vocation</td>\n",
              "      <td>vocat</td>\n",
              "      <td>vocat</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15607</th>\n",
              "      <td>volition</td>\n",
              "      <td>volition</td>\n",
              "      <td>volit</td>\n",
              "      <td>volit</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15608</th>\n",
              "      <td>westernisation</td>\n",
              "      <td>occidentalisation</td>\n",
              "      <td>westernis</td>\n",
              "      <td>occidentalis</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15609</th>\n",
              "      <td>workstation</td>\n",
              "      <td>poste de travail</td>\n",
              "      <td>workstat</td>\n",
              "      <td>poste de travail</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15610</th>\n",
              "      <td>zonation</td>\n",
              "      <td>zonage</td>\n",
              "      <td>zonat</td>\n",
              "      <td>zonag</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15611 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  ENG                 FR  ...           fr_stem ratio_similarity\n",
              "0               aback       décontenancé  ...       décontenanc               25\n",
              "1             airlock                sas  ...               sas               20\n",
              "2             airsick    le mal de l'air  ...   le mal de l'air               27\n",
              "3               alack          un manque  ...          un manqu               15\n",
              "4                amok               amok  ...              amok              100\n",
              "...               ...                ...  ...               ...              ...\n",
              "15606        vocation           vocation  ...             vocat              100\n",
              "15607        volition           volition  ...             volit              100\n",
              "15608  westernisation  occidentalisation  ...      occidentalis               38\n",
              "15609     workstation   poste de travail  ...  poste de travail               33\n",
              "15610        zonation             zonage  ...             zonag               80\n",
              "\n",
              "[15611 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd5aDmpPM033"
      },
      "source": [
        "main_df = main_df.drop_duplicates()\n",
        "main_dict = dict(zip(main_df.FR,main_df.ratio_similarity))\n",
        "main_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khYcetEBKY4I"
      },
      "source": [
        "## DECEPTIVE COGNOMES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RRl_kynKVxi"
      },
      "source": [
        "main_Dcognomes= pd.read_csv('https://raw.githubusercontent.com/mbayle98/S2_project/main/BIGSCALE/Cognomes_df.txt', delimiter = \"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpxqF8MZScx_"
      },
      "source": [
        "main_Dcognomes= pd.read_csv('https://raw.githubusercontent.com/mbayle98/S2_project/main/BIGSCALE/Cognomes_df.txt', delimiter = \"\\t\")\n",
        "\n",
        "main_Dcognomes['eng_stem'] = main_Dcognomes['English'].apply(lambda x: stemmer.stem(x))\n",
        "main_Dcognomes['fr_stem'] = main_Dcognomes['Français'].apply(lambda x: stemmerfr.stem(x))\n",
        "main_Dcognomes['ratio_similarity'] = main_Dcognomes.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "main_Dcognomes_dict = dict(zip(main_Dcognomes.Français,main_Dcognomes.ratio_similarity))\n",
        "main_Dcognomes_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNth7P1-Yj84"
      },
      "source": [
        "main_Dcognomes_dict = dict(zip(main_Dcognomes.Français,main_Dcognomes.ratio_similarity))\n",
        "main_Dcognomes_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bke8bfwVY7PO"
      },
      "source": [
        "Phrase =\"Aujourd'hui j'ai fait de la géométrie, nous avons basique solide étudié la translation d'un plan, mon professeur nous surveillait et nous donnait des avis sur notre travail du jour\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9ruEUbmdbHy"
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1dp6w55Zc6g"
      },
      "source": [
        "def Check_Dcognomes(sentences):\n",
        "    sentence_clean = \"\"\n",
        "    lemm_sentence = lemma_french_token(sentences)\n",
        "    doc = TreebankWordDetokenizer().detokenize(lemm_sentence)\n",
        "    doc = nlp(doc)\n",
        "\n",
        "    for token in doc:\n",
        "      if token.pos_ == 'NOUN'or token.pos_ == 'VERB' or token.pos_ =='ADJ':\n",
        "        sentence_clean = sentence_clean + ' ' + token.text\n",
        "      \n",
        "    nb_Dcognomes = 0\n",
        "    sentence = sentence_clean.split()\n",
        "    for i in sentence:\n",
        "      if i in main_Dcognomes_dict:\n",
        "        nb_Dcognomes = nb_Dcognomes + 1\n",
        "    \n",
        "    return nb_Dcognomes\n",
        "  \n",
        "df1['Nb_Dcognomes'] = df1['lemma'].apply(lambda x: Check_Dcognomes(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IM60sj3ekHT"
      },
      "source": [
        "def count_DeceptiveCognome_and_similarity_score(sentences):\n",
        "\n",
        "  sentence_clean = \"\"\n",
        "  lemm_sentence = lemma_french_token(sentences)\n",
        "  doc = TreebankWordDetokenizer().detokenize(lemm_sentence)\n",
        "  doc = nlp(doc)\n",
        "\n",
        "  for token in doc:\n",
        "    if token.pos_ == 'NOUN'or token.pos_ == 'VERB' or token.pos_ =='ADJ':\n",
        "      sentence_clean = sentence_clean + ' ' + token.text\n",
        "\n",
        "  similarity_score_Dcognomes = 0\n",
        "  similarity_score_tt = 0\n",
        "  list_words = []\n",
        "  doc = sentence_clean.split()\n",
        "\n",
        "  for i in doc:\n",
        "    if i in main_Dcognomes_dict.keys():\n",
        "      similarity_score_Dcognomes = similarity_score_Dcognomes + main_Dcognomes_dict[i]\n",
        "      \n",
        "    if i in main_dict.keys():\n",
        "      similarity_score_tt = similarity_score_tt + main_dict[i]\n",
        "\n",
        "    final_score = similarity_score_tt - similarity_score_Dcognomes\n",
        "    return  final_score\n",
        "\n",
        "df1['score'] = df1['lemma'].apply(lambda x: count_DeceptiveCognome_and_similarity_score(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySIyRMC2Ksym"
      },
      "source": [
        "## 3/ AutoML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjGs2vAmK28P"
      },
      "source": [
        "df1['type'] = df.apply(lambda _: '', axis = 1)\n",
        "df6 = pd.DataFrame()\n",
        "df6['type'] = df1['type']\n",
        "df6[['sentence', 'Level']] = df1[['sentences_cleaned', 'Level']]\n",
        "df6.update('\"' + df6['sentence'].astype(str) + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "sMYeOgWBbb8S",
        "outputId": "dc333059-de9a-4ff2-b82d-ebece0c79c6b"
      },
      "source": [
        "df6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>sentence</th>\n",
              "      <th>Level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>\"formation achèvera par contrôle pour savoir p...</td>\n",
              "      <td>B2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>\"des sanctions administratives des ordonnances...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>\"canidé âge avancé été bien malgré lui coeur c...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td>\"tout savoir avant adopter chat\"</td>\n",
              "      <td>A2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>\"chat nouveau meilleur ami homme\"</td>\n",
              "      <td>A2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td></td>\n",
              "      <td>\"savait que souvenir même piano faussait encor...</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td></td>\n",
              "      <td>\"avais pas plus grand désir que voir une tempê...</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td></td>\n",
              "      <td>\"car souvent dans une trouve égaré jour une au...</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td></td>\n",
              "      <td>\"souffle vent les dispersait pierre était nouv...</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td></td>\n",
              "      <td>\"plus tard arrive que devenus habiles dans cul...</td>\n",
              "      <td>C2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1030 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     type                                           sentence Level\n",
              "0          \"formation achèvera par contrôle pour savoir p...    B2\n",
              "1          \"des sanctions administratives des ordonnances...    C1\n",
              "2          \"canidé âge avancé été bien malgré lui coeur c...    C1\n",
              "3                           \"tout savoir avant adopter chat\"    A2\n",
              "4                          \"chat nouveau meilleur ami homme\"    A2\n",
              "...   ...                                                ...   ...\n",
              "1025       \"savait que souvenir même piano faussait encor...    C2\n",
              "1026       \"avais pas plus grand désir que voir une tempê...    C2\n",
              "1027       \"car souvent dans une trouve égaré jour une au...    C2\n",
              "1028       \"souffle vent les dispersait pierre était nouv...    C2\n",
              "1029       \"plus tard arrive que devenus habiles dans cul...    C2\n",
              "\n",
              "[1030 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lNpj5ahyQD4-",
        "outputId": "3a9287d3-d601-4705-fead-ba2b77266605"
      },
      "source": [
        "import csv \n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "df6.to_csv(\"finaaaaal.csv\", encoding= 'utf-8-sig' , sep = ',', index = False, quoting = csv.QUOTE_NONE )\n",
        "files.download('finaaaaal.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e53d6167-9a1a-4adf-ad49-f8b2c5e1ca9b\", \"finaaaaal.csv\", 146974)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
