{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Cleaned_functions_bsa.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UoqIGtABJYwc"
      ],
      "authorship_tag": "ABX9TyMIY2kpeLmNFDvwucQiSvuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epicalekspwner/BigScaleAnalytics2021/blob/main/Functions_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoqIGtABJYwc"
      },
      "source": [
        "## 1/Libraries and lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wz0bXwZBdky"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from collections import OrderedDict\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9WVY0G7JGvW",
        "outputId": "005e07b6-291d-4129-f400-5b5677ba7430"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rr9GaNqtBN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1ec523-4301-46d4-fdf0-1bf48c717db7"
      },
      "source": [
        "pip install spacy-lefff"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-lefff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/21/a326ed06c4175ec07baf61ee563dbc5bc06637e5d863f11ce9409d007203/spacy-lefff-0.4.0.tar.gz (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 8.9MB/s \n",
            "\u001b[?25hCollecting spacy<3.0.5,>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/c4/a5a8aa936e1b7fbba1780863447a51684dabb7f8855931f2510d4637d641/spacy-3.0.4-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 27.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.19.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.11.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.8.2)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Collecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (56.0.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (20.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (2.0.5)\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.7MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.5,>=3.0.0->spacy-lefff) (0.4.1)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 48.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (7.1.2)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.0.5,>=3.0.0->spacy-lefff) (2.4.7)\n",
            "Building wheels for collected packages: spacy-lefff, smart-open\n",
            "  Building wheel for spacy-lefff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lefff: filename=spacy_lefff-0.4.0-cp37-none-any.whl size=2929893 sha256=d2c1b1b6f9371a255c45121f1c09ea036ce79344232e61de88c51b9b5fb43648\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/1c/f0/9b95e4e74005afbfe54aa126484febacf0fd27feffa3e9ad45\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=de0c1c1364056f651c1802a8ed0af9e091a524b7960b142e61da1a07314863ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built spacy-lefff smart-open\n",
            "Installing collected packages: catalogue, spacy-legacy, typer, smart-open, pathy, srsly, pydantic, thinc, spacy, spacy-lefff\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.4 spacy-lefff-0.4.0 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak_u5r4kBnwS",
        "outputId": "da0497d7-222b-4b89-fcff-b2cea3e33488"
      },
      "source": [
        "# Spacy library\n",
        "\n",
        "!python -m spacy download fr_core_news_md\n",
        "\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import spacy\n",
        "from spacy_lefff import LefffLemmatizer\n",
        "from spacy.language import Language\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_md\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-03 17:27:03.860436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting fr-core-news-md==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.0.0/fr_core_news_md-3.0.0-py3-none-any.whl (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 99kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (20.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (56.0.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->fr-core-news-md==3.0.0) (2.4.7)\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LJL0FB9Sog5"
      },
      "source": [
        "# NLTK --> Stemming\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmerfr = SnowballStemmer(\"french\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppj2_EpVSvmF",
        "outputId": "1c376bb6-fa25-4c57-d833-7414fbb927f8"
      },
      "source": [
        "pip install deep-translator"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b3/53b50057bf1a5ebbb8a77c19c42ae721129ebe1b8478029f2121a5914f07/deep_translator-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->deep-translator) (1.24.3)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNfQH0hMTqw2"
      },
      "source": [
        "from deep_translator import GoogleTranslator"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bQGWOImcHij"
      },
      "source": [
        "# Special lists/dict of char\n",
        "\n",
        "liste_special_letters = ['é','è','ê','ë','à','ä','â','ï','î','ö','ô','ü','ù','û','ç']\n",
        "contractions_dict = { \"d'\": \"de \",\"n'\":\"ne \",\"l'\":\"le \",\"s'\":\"se \",\"c'\":\"ce \",\"j'\":\"je \",\"t'\":\"tu \",\"qu'\":\"que \"}\n",
        "stopWord = ['le','la','les','de','des','se','ce','à']\n",
        "Fr_stp_modif = {'a', 'abord', 'afin', 'ah', 'ai', 'aie', 'ainsi',\n",
        " 'alors', 'anterieur', 'anterieure', 'anterieures', 'apres', 'après', \n",
        " 'assez',  'au', 'aucun', 'aucune', 'aujourd', \"aujourd'hui\", 'aupres',\n",
        " 'auquel', 'aussi', 'autre', 'autrement', 'autres', 'autrui',\n",
        " 'aux',  'auxquelles', 'auxquels',  'avant', 'avec',  'bas', 'basee', 'car', 'ce', 'ceci',\n",
        " 'cela', 'celle', 'celle-ci', 'celle-là', 'celles', 'celles-ci', 'celles-là', 'celui',\n",
        " 'celui-ci', 'celui-là', 'cent', 'cependant', 'certain', 'certaine', 'certaines', 'certains', 'certes', 'ces', 'cet', 'cette', 'ceux',\n",
        " 'ceux ci', 'ceux là', 'chacun', 'chacune', 'chaque', 'chez', 'ci', 'cinq', 'cinquantaine', 'cinquante', 'cinquantième', 'cinquième',\n",
        " 'combien', 'comme', 'comment', 'compris', 'concernant', 'da', 'dans', 'de', 'debout', 'dedans', 'dehors', 'deja', 'delà', 'depuis',\n",
        " 'derriere', 'derrière', 'des', 'desormais', 'desquelles', 'desquels', 'dessous', 'dessus', 'deux', 'deuxième', 'deuxièmement', 'devant',\n",
        " 'devers', 'devra', 'different', 'differentes', 'differents', 'différent', 'différente', 'différentes', 'différents', 'dire', 'directe',\n",
        " 'directement', 'divers', 'diverse', 'diverses', 'dix', 'dix huit', 'dix neuf', 'dix sept', 'dixième', 'donc', 'dont', 'douze',\n",
        " 'douzième', 'du', 'duquel', 'durant','dès', 'désormais', 'effet','egale', 'egalement', 'egales', 'eh', 'elle', 'elle même',\n",
        " 'elles', 'elles mêmes', 'en', 'encore', 'enfin', 'entre','envers', 'environ','et', 'etc', 'eux', 'eux mêmes',\n",
        " 'exactement',\n",
        " 'excepté',\n",
        "  'chat'\n",
        "  'chien'\n",
        " 'façon',\n",
        "\n",
        " 'gens',\n",
        " 'ha',\n",
        " 'hem',\n",
        " 'hep',\n",
        " 'hi',\n",
        " 'ho',\n",
        " 'hormis',\n",
        " 'hors',\n",
        " 'hou',\n",
        " 'houp',\n",
        " 'hue',\n",
        " 'hui',\n",
        " 'huit',\n",
        " 'huitième',\n",
        " 'hé',\n",
        " 'i',\n",
        " 'il',\n",
        " 'ils',\n",
        " 'importe',\n",
        " \"j \",\n",
        " 'je',\n",
        " 'jusqu',\n",
        " 'jusque',\n",
        " 'juste',\n",
        " 'j ',\n",
        " \"l \",\n",
        " 'la',\n",
        " 'laquelle',\n",
        " 'le',\n",
        " 'lequel',\n",
        " 'les',\n",
        " 'lesquelles',\n",
        " 'lesquels',\n",
        " 'leur',\n",
        " 'leurs',\n",
        " 'longtemps',\n",
        " 'lors',\n",
        " 'lorsque',\n",
        " 'lui',\n",
        " 'lui meme',\n",
        " 'lui même',\n",
        " 'là',\n",
        " 'lès',\n",
        " 'l ',\n",
        " \"m \",\n",
        " 'ma',\n",
        " 'maint',\n",
        " 'maintenant',\n",
        " 'mais',\n",
        " 'malgré',\n",
        " 'me',\n",
        " 'meme',\n",
        " 'memes',\n",
        " 'merci',\n",
        " 'mes',\n",
        " 'mien',\n",
        " 'mienne',\n",
        " 'miennes',\n",
        " 'miens',\n",
        " 'mille',\n",
        " 'moi',\n",
        " 'moi meme',\n",
        " 'moi même',\n",
        " 'moindres',\n",
        " 'moins',\n",
        " 'mon',\n",
        " 'même',\n",
        " 'mêmes',\n",
        " 'm ',\n",
        " \"n \",\n",
        " 'na',\n",
        " 'ne',\n",
        " 'neanmoins',\n",
        " 'neuvième',\n",
        " 'ni',\n",
        " 'nombreuses',\n",
        " 'nombreux',\n",
        " 'nos',\n",
        " 'notamment',\n",
        " 'notre',\n",
        " 'nous',\n",
        " 'nous-mêmes',\n",
        " 'nouvea',\n",
        " 'nul',\n",
        " 'néanmoins',\n",
        " 'nôtre',\n",
        " 'nôtres',\n",
        " 'n’',\n",
        " 'o',\n",
        " 'on',\n",
        " \n",
        " 'onze',\n",
        " 'onzième',\n",
        " 'ore',\n",
        " 'ou',\n",
        " 'ouias',\n",
        " 'oust',\n",
        " 'outre',\n",
        "\n",
        " 'ouverte',\n",
        " 'ouverts',\n",
        " 'où',\n",
        " 'par',\n",
        " 'parce',\n",
        " 'parfois',\n",
        " 'parmi',\n",
        " 'parseme',\n",
        " 'partant',\n",
        " 'pas',\n",
        " 'pendant',\n",
        " 'pense',\n",
        " 'permet',\n",
        " 'personne',\n",
        " 'peu',\n",
        "\n",
        " 'plus',\n",
        " 'plusieurs',\n",
        " 'plutôt',\n",
        " 'possible',\n",
        " 'possibles',\n",
        " 'pour',\n",
        " 'pourquoi',\n",
        "\n",
        " 'prealable',\n",
        " 'precisement',\n",
        " 'premier',\n",
        " 'première',\n",
        " 'premièrement',\n",
        " 'pres',\n",
        " 'procedant',\n",
        " 'proche',\n",
        " 'près',\n",
        " 'pu',\n",
        " 'puis',\n",
        " 'puisque',\n",
        " \"qu'\",\n",
        " 'quand',\n",
        " 'quant',\n",
        " 'quant à soi',\n",
        " 'quanta',\n",
        " 'quarante',\n",
        " 'quatorze',\n",
        " 'quatre',\n",
        " 'quatre vingt',\n",
        " 'quatrième',\n",
        " 'quatrièmement',\n",
        " 'que',\n",
        " 'quel',\n",
        " 'quelconque',\n",
        " 'quelle',\n",
        " 'quelles',\n",
        " \"quelqu'un\",\n",
        " 'quelque',\n",
        " 'quelques',\n",
        " 'quels',\n",
        " 'qui',\n",
        " 'quiconque',\n",
        " 'quinze',\n",
        " 'quoi',\n",
        " 'quoique',\n",
        " 'qu’',\n",
        " 'relative',\n",
        " 'relativement',\n",
        " \n",
        "\n",
        " 'retour',\n",
        " 'revoici',\n",
        " 'revoilà',\n",
        " \"s \",\n",
        " 'sa',\n",
        " 'sait',\n",
        " 'sans',\n",
        " 'sauf',\n",
        " 'se',\n",
        " 'seize',\n",
        " 'selon',\n",
        " 'semblable',\n",
        " 'semblaient',\n",
        " 'semble',\n",
        " 'semblent',\n",
        " 'sent',\n",
        " 'sept',\n",
        " 'septième',\n",
        "\n",
        " 'ses',\n",
        " 'seul',\n",
        " 'seule',\n",
        " 'seulement',\n",
        " 'si',\n",
        " 'sien',\n",
        " 'sienne',\n",
        " 'siennes',\n",
        " 'siens',\n",
        " 'sinon',\n",
        " 'six',\n",
        " 'sixième',\n",
        " 'soi',\n",
        " 'soi même',\n",
        " 'soit',\n",
        " 'soixante',\n",
        " 'son',\n",
        "\n",
        " 'sous',\n",
        " 'souvent',\n",
        " 'specifique',\n",
        " 'specifiques',\n",
        " 'stop',\n",
        " 'suffisant',\n",
        " 'suffisante',\n",
        "\n",
        " 'suivant',\n",
        " 'suivante',\n",
        " 'suivantes',\n",
        " 'suivants',\n",
        " 'suivre',\n",
        " 'sur',\n",
        " 'surtout',\n",
        " 's ',\n",
        " \"t \",\n",
        " 'ta',\n",
        " 'tant',\n",
        " 'te',\n",
        " 'tel',\n",
        " 'telle',\n",
        " 'tellement',\n",
        " 'telles',\n",
        " 'tels',\n",
        " 'tenant',\n",
        "\n",
        " 'tente',\n",
        " 'tes',\n",
        " 'tien',\n",
        " 'tienne',\n",
        " 'tiennes',\n",
        " 'tiens',\n",
        " 'toi',\n",
        " 'toi-même',\n",
        " 'ton',\n",
        " 'touchant',\n",
        " 'toujours',\n",
        " 'tous',\n",
        " 'tout',\n",
        " 'toute',\n",
        " 'toutes',\n",
        " 'treize',\n",
        " 'trente',\n",
        " 'tres',\n",
        " 'trois',\n",
        " 'troisième',\n",
        " 'troisièmement',\n",
        " 'tu',\n",
        " 'té',\n",
        " 't ',\n",
        " 'un',\n",
        " 'une',\n",
        " 'unes',\n",
        " 'uns',\n",
        " 'va',\n",
        " 'vais',\n",
        " 'vas',\n",
        " 'vers',\n",
        " 'via',\n",
        " 'vingt',\n",
        " 'voici',\n",
        " 'voilà',\n",
        " 'vont',\n",
        " 'vos',\n",
        " 'votre',\n",
        " 'vous',\n",
        " 'vous mêmes',\n",
        " 'vu',\n",
        " 'vé',\n",
        " 'vôtre',\n",
        " 'vôtres',\n",
        " 'à',\n",
        " 'â',\n",
        " 'ça',\n",
        " 'ès',\n",
        " 'ô'}\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzq3iMAtLrau"
      },
      "source": [
        "name_female = pd.read_csv('https://gist.githubusercontent.com/dmassiani/c669d79aa27b93cf03b8f0eb5088bde6/raw/52fad81fa091b23a812259e114bb5f11f9691ade/Liste%2520des%2520pr%25C3%25A9noms%2520fran%25C3%25A7ais%2520f%25C3%25A9minins', header=None)\n",
        "name_male = pd.read_csv('https://gist.githubusercontent.com/dmassiani/d3ad74eed7903dd2e36fb71b0d5de39e/raw/fa34fd3d006171ae92842e3ec7b06647609f726f/Liste%2520des%2520pr%25C3%25A9noms%2520fran%25C3%25A7ais%2520masculins', header= None)\n",
        "name_list = name_female.values.tolist()\n",
        "name_list.append(name_male.values.tolist())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR5bRRCjOXGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2d6a80-a58b-419d-9552-44cb8bc0b1c1"
      },
      "source": [
        "len(name_list)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "459"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b9LBUAlpDQr"
      },
      "source": [
        "## 2/ Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S2fxHFSJhFl"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS2VTNEcJGMV"
      },
      "source": [
        "# Dataframe with french sentences and level\n",
        "\n",
        "Phrases = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Phrasee.txt?token=AQ7V5XWIHMNRER262JJJ6YTATEP5A', delimiter = \"\\t\")\n",
        "Niveau = pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/Niveau.txt?token=AQ7V5XTREN66QIT2FRKGL5LATEP64', delimiter = \"\\t\" )\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['Level'] = Niveau['Type (Maxime)']\n",
        "df['Sentences'] = Phrases['Phrases']\n",
        "\n",
        "#NOTE:to apply these functions make sure that the language level are called \"Level\" and the sentences \"Sentences\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqDwMaoAaz2P"
      },
      "source": [
        "def cleaning_general(df):\n",
        "\n",
        "    # Data cleaning 1 --> char replacement + lower\n",
        "\n",
        "    df['Sentences'] = df['Sentences'].apply(lambda x: x.replace(\"’\",\"'\"))\n",
        "    df['Sentences'] = df['Sentences'].apply(lambda x: x.lower())\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 2 --> Contractions expanding\n",
        "\n",
        "    contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "        # Function for expanding contractions\n",
        "\n",
        "    def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "      def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "      return contractions_re.sub(replace, text)\n",
        "\n",
        "    df['Cleaning1'] = df['Sentences'].apply(lambda x: expand_contractions(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 3 --> Punctuation removal\n",
        "\n",
        "    def punctuation_removal(text):\n",
        "        all_list = [char for char in text if char not in string.punctuation]\n",
        "        clean_str = ''.join(all_list)\n",
        "        return clean_str\n",
        "\n",
        "    df['Cleaning2'] = df['Cleaning1'].apply(lambda x: punctuation_removal(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 4 --> Remove special char (took into account french special char e.g. \"ç\",\"é\")\n",
        "\n",
        "    def bin_spe(tweet):\n",
        "        tweet = ' '.join(re.sub(\"[^0-9a-zÀ-ÿ-A-Z-ç \\t]\",\" \", tweet).split())\n",
        "        return tweet\n",
        "\n",
        "    df['Cleaning3'] = df['Cleaning2'].apply(lambda x: bin_spe(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 5 --> Stop words removal and names\n",
        "\n",
        "    def stopword_removal(text):\n",
        "        all_list = [char for char in text if char not in stopWord ]\n",
        "        all_list = [char for char in text if char not in name_list]\n",
        "        clean_str = ''.join(all_list)\n",
        "        return clean_str\n",
        "\n",
        "    df['Cleaning3'] = df['Cleaning2'].apply(lambda x: stopword_removal(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Data cleaning 6 --> remove alone letter and words with len() ==2\n",
        "\n",
        "    def remove_alone_letters_and_len2(texte):\n",
        "        texte = ' '.join(i for i in texte.split() if not len(i) == 1 and not len(i) == 2)\n",
        "        return texte\n",
        "\n",
        "    df['Cleaning4'] = df['Cleaning3'].apply(lambda x: remove_alone_letters_and_len2(x))\n",
        "\n",
        "    return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhMTFpyObspN"
      },
      "source": [
        "def lemma_and_Leff(df1):\n",
        "    \n",
        "    \n",
        "    # Count top 100 words whole dataset\n",
        "\n",
        "    Counter(\" \".join(df1[\"Cleaning4\"]).split()).most_common(100)\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Lemmatization --> OUTPUT: Token\n",
        "\n",
        "    def lemma_french_token(text):\n",
        "      doc = nlp(text)\n",
        "      liste1 = []\n",
        "      for token in doc:\n",
        "        liste1.append(token.lemma_)\n",
        "      return liste1\n",
        "\n",
        "    df1['lemma'] = df1['Cleaning4'].apply(lambda x: lemma_french_token(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Lemmatization --> OUTPUT: sentence\n",
        "\n",
        "    def lemma_french_full_sentence(text):\n",
        "      doc = nlp(text)\n",
        "      lemmatized_output = ' '.join([w.lemma_ for w in doc])\n",
        "      return lemmatized_output\n",
        "\n",
        "    df1['lemma'] = df1['Cleaning4'].apply(lambda x: lemma_french_full_sentence(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Count to 100 words after lemma whole dataset\n",
        "\n",
        "    Top_100_lemma = Counter(\" \".join(df1[\"lemma\"]).split()).most_common(100)\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    # Get a list of the top 100 words in the dataset\n",
        "\n",
        "    def count_to_list(my_list_count_100):\n",
        "      list2 = []\n",
        "      for my_tuple in my_list_count_100:\n",
        "          list2.append(my_tuple[0])\n",
        "      return list2\n",
        "\n",
        "    List_top_100 = count_to_list(Top_100_lemma)\n",
        "\n",
        "\n",
        "    List_top_100\n",
        "\n",
        "    # Data cleaning 7 --> Common word removal\n",
        "\n",
        "    def common_word_removal(text):\n",
        "\n",
        "        all_list =  \" \".join([word for word in text.split() if word not in List_top_100])\n",
        "        \n",
        "        return all_list\n",
        "\n",
        "    df1['Cleaning7'] = df1['lemma'].apply(lambda x: common_word_removal(x))\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "    def List_STP_word_removal(text):\n",
        "\n",
        "        all_list =  \" \".join([word for word in text.split() if word not in Fr_stp_modif])\n",
        "        \n",
        "        return all_list\n",
        "\n",
        "    df1['Cleaning8'] = df1['Cleaning7'].apply(lambda x: List_STP_word_removal(x))\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#Create dict that returns the type of the word as value and as key the word {\"word\" : \"Type\"}\n",
        "\n",
        "    def POS_french_token(sentences):\n",
        "        dict1 = {}\n",
        "        doc = nlp(sentences)\n",
        "        for token in doc:\n",
        "          dict1.update({token.text : token.pos_})\n",
        "        return dict1\n",
        "\n",
        "    df1['POS_LEFFF2'] = df1['lemma'].apply(lambda x: POS_french_token(x))\n",
        "    df1['POS_without_lemma'] = df1['Cleaning4'].apply(lambda x: POS_french_token(x))\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    #Get a list from dict with all the verbs\n",
        "\n",
        "    def get_dict_value_verb(POS_dict):\n",
        "          \n",
        "        liste_verb =[]\n",
        "\n",
        "        for key, value in POS_dict.items():\n",
        "          \n",
        "          if value == 'VERB':\n",
        "             liste_verb.append(key)\n",
        "        return liste_verb\n",
        "\n",
        "\n",
        "    df1['Listes_verb'] = df1['POS_LEFFF2'].apply(lambda x: get_dict_value_verb(x))\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Get a liste from dict with all the nouns\n",
        "\n",
        "\n",
        "    def get_dict_value_noun(POS_dict):\n",
        "          \n",
        "        liste_noun =[]\n",
        "\n",
        "        for key, value in POS_dict.items():\n",
        "          \n",
        "            if value == 'NOUN':\n",
        "               liste_noun.append(key)\n",
        "        return liste_noun\n",
        "\n",
        "\n",
        "    df1['Listes_noun'] = df1['POS_LEFFF2'].apply(lambda x: get_dict_value_noun(x))\n",
        "\n",
        "\n",
        "    return df1\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9mpTl73bXB"
      },
      "source": [
        "# Final functions that applied the cleaning functions and the lemma and POS functions\n",
        "\n",
        "def cleaning_Leff(df):\n",
        "  cleaning_general(df)\n",
        "  lemma_and_Leff(df)\n",
        "  return df\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE1euVl02LGg"
      },
      "source": [
        "# Transform list of list into one single list + remove duplicate nouns\n",
        "\n",
        "def flat_list_noun(df1):\n",
        "\n",
        "    cleaning_Leff(df1)\n",
        "    liste_noun = df1['Listes_noun'].tolist()\n",
        "    flat_list_noun = list(itertools.chain(*liste_noun))\n",
        "    flat_list_noun = sorted(flat_list_noun,reverse=True)\n",
        "    flat_list_noun = list(OrderedDict.fromkeys(flat_list_noun))\n",
        "\n",
        "    return flat_list_noun\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def flat_list_verb(df1):\n",
        "\n",
        "    cleaning_Leff(df1)\n",
        "    liste_verb = df1['Listes_verb'].tolist()\n",
        "    flat_list_verb = list(itertools.chain(*liste_verb))\n",
        "    flat_list_verb = sorted(flat_list_verb,reverse=True)\n",
        "    flat_list_verb = list(OrderedDict.fromkeys(flat_list_verb))\n",
        "\n",
        "    return flat_list_verb\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31i21uOe6ryc"
      },
      "source": [
        "## DEAL WITH DECEPTIVE COGNOMES"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyf-wv8Hfm53"
      },
      "source": [
        "# Similarity check\n",
        "\n",
        "import difflib\n",
        "\n",
        "def stem_checker(stem1,stem2):\n",
        "\n",
        "  sequence = round(difflib.SequenceMatcher(None,stem1,stem2).ratio()*100)\n",
        "  return sequence \n",
        "\n",
        "# https://www.kite.com/python/docs/difflib.SequenceMatcher\n",
        "\n",
        "#-------------------------------------------------------------------------------"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfpuVsZOfQ9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "473f1561-1a36-4c0b-dd73-8798a0a90c9e"
      },
      "source": [
        "# Dataframe with Deceptive cognomes (false friends) \n",
        "\n",
        "main_Dcognomes= pd.read_csv('https://raw.githubusercontent.com/mbayle98/S2_project/main/BIGSCALE/Cognomes_df.txt', delimiter = \"\\t\")\n",
        "\n",
        "main_Dcognomes['eng_stem'] = main_Dcognomes['English'].apply(lambda x: stemmer.stem(x))\n",
        "main_Dcognomes['fr_stem'] = main_Dcognomes['Français'].apply(lambda x: stemmerfr.stem(x))\n",
        "main_Dcognomes['ratio_similarity'] = main_Dcognomes.apply(lambda x: stem_checker(x['eng_stem'],x['fr_stem']),axis = 1)\n",
        "\n",
        "main_Dcognomes_dict = dict(zip(main_Dcognomes.Français,main_Dcognomes.ratio_similarity))\n",
        "main_Dcognomes"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Français</th>\n",
              "      <th>eng_stem</th>\n",
              "      <th>fr_stem</th>\n",
              "      <th>ratio_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abuse</td>\n",
              "      <td>insulter</td>\n",
              "      <td>abus</td>\n",
              "      <td>insult</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>academic</td>\n",
              "      <td>universitaire</td>\n",
              "      <td>academ</td>\n",
              "      <td>universitair</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>accomodate</td>\n",
              "      <td>loger</td>\n",
              "      <td>accomod</td>\n",
              "      <td>log</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>actual</td>\n",
              "      <td>réel</td>\n",
              "      <td>actual</td>\n",
              "      <td>réel</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>actual</td>\n",
              "      <td>effectif</td>\n",
              "      <td>actual</td>\n",
              "      <td>effect</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>shade</td>\n",
              "      <td>store</td>\n",
              "      <td>shade</td>\n",
              "      <td>stor</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>implore</td>\n",
              "      <td>supplier</td>\n",
              "      <td>implor</td>\n",
              "      <td>suppli</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>supervise</td>\n",
              "      <td>surveiller</td>\n",
              "      <td>supervis</td>\n",
              "      <td>surveil</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>translation</td>\n",
              "      <td>translation géométrique</td>\n",
              "      <td>translat</td>\n",
              "      <td>translation géometr</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>capricious</td>\n",
              "      <td>versatile</td>\n",
              "      <td>caprici</td>\n",
              "      <td>versatil</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>145 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         English  ... ratio_similarity\n",
              "0          abuse  ...               20\n",
              "1       academic  ...               11\n",
              "2     accomodate  ...               20\n",
              "3         actual  ...               20\n",
              "4         actual  ...               33\n",
              "..           ...  ...              ...\n",
              "140        shade  ...               22\n",
              "141      implore  ...               33\n",
              "142    supervise  ...               67\n",
              "143  translation  ...               59\n",
              "144   capricious  ...               27\n",
              "\n",
              "[145 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQ8nmC17MYa"
      },
      "source": [
        "## DEAL WITH COGNOMES FINAL DATAFRAME"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7xsyGHkfaNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "a82a113f-0eca-414f-c8d8-41719e47fa83"
      },
      "source": [
        "main_df= pd.read_csv('https://raw.githubusercontent.com/epicalekspwner/BigScaleAnalytics2021/main/Datasets/MAINDF_.txt?token=AQ7V5XWU4PMHCIPYGZ4XRJ3ATEQBQ', delimiter = \"\\t\")\n",
        "main_df = main_df.drop_duplicates()\n",
        "main_dict = dict(zip(main_df.FR,main_df.ratio_similarity))\n",
        "main_df"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ENG</th>\n",
              "      <th>FR</th>\n",
              "      <th>eng_stem</th>\n",
              "      <th>fr_stem</th>\n",
              "      <th>ratio_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aback</td>\n",
              "      <td>décontenancé</td>\n",
              "      <td>aback</td>\n",
              "      <td>décontenanc</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>airlock</td>\n",
              "      <td>sas</td>\n",
              "      <td>airlock</td>\n",
              "      <td>sas</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>airsick</td>\n",
              "      <td>le mal de l'air</td>\n",
              "      <td>airsick</td>\n",
              "      <td>le mal de l'air</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alack</td>\n",
              "      <td>un manque</td>\n",
              "      <td>alack</td>\n",
              "      <td>un manqu</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>amok</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15606</th>\n",
              "      <td>vocation</td>\n",
              "      <td>vocation</td>\n",
              "      <td>vocat</td>\n",
              "      <td>vocat</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15607</th>\n",
              "      <td>volition</td>\n",
              "      <td>volition</td>\n",
              "      <td>volit</td>\n",
              "      <td>volit</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15608</th>\n",
              "      <td>westernisation</td>\n",
              "      <td>occidentalisation</td>\n",
              "      <td>westernis</td>\n",
              "      <td>occidentalis</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15609</th>\n",
              "      <td>workstation</td>\n",
              "      <td>poste de travail</td>\n",
              "      <td>workstat</td>\n",
              "      <td>poste de travail</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15610</th>\n",
              "      <td>zonation</td>\n",
              "      <td>zonage</td>\n",
              "      <td>zonat</td>\n",
              "      <td>zonag</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14209 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  ENG                 FR  ...           fr_stem ratio_similarity\n",
              "0               aback       décontenancé  ...       décontenanc               25\n",
              "1             airlock                sas  ...               sas               20\n",
              "2             airsick    le mal de l'air  ...   le mal de l'air               27\n",
              "3               alack          un manque  ...          un manqu               15\n",
              "4                amok               amok  ...              amok              100\n",
              "...               ...                ...  ...               ...              ...\n",
              "15606        vocation           vocation  ...             vocat              100\n",
              "15607        volition           volition  ...             volit              100\n",
              "15608  westernisation  occidentalisation  ...      occidentalis               38\n",
              "15609     workstation   poste de travail  ...  poste de travail               33\n",
              "15610        zonation             zonage  ...             zonag               80\n",
              "\n",
              "[14209 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYVgHRZ5AIH6"
      },
      "source": [
        "def lemma_french_token(text):\n",
        "  doc = nlp(text)\n",
        "  liste1 = []\n",
        "  for token in doc:\n",
        "    liste1.append(token.lemma_)\n",
        "  return liste1"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf11Nw_heSYP"
      },
      "source": [
        "def Check_Dcognomes(sentences):\n",
        "  \n",
        "    sentence_clean = \"\"\n",
        "    lemm_sentence = lemma_french_token(sentences)\n",
        "    doc = TreebankWordDetokenizer().detokenize(lemm_sentence)\n",
        "    doc = nlp(doc)\n",
        "\n",
        "    for token in doc:\n",
        "      if token.pos_ == 'NOUN'or token.pos_ == 'VERB' or token.pos_ =='ADJ':\n",
        "        sentence_clean = sentence_clean + ' ' + token.text\n",
        "      \n",
        "    nb_Dcognomes = 0\n",
        "    sentence = sentence_clean.split()\n",
        "    for i in sentence:\n",
        "      if i in main_Dcognomes_dict:\n",
        "        nb_Dcognomes = nb_Dcognomes + 1\n",
        "    \n",
        "    \n",
        "    return nb_Dcognomes"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqSvr2D5ekFl"
      },
      "source": [
        "def count_DeceptiveCognome_and_similarity_score(sentences):\n",
        "\n",
        "  sentence_clean = \"\"\n",
        "  lemm_sentence = lemma_french_token(sentences)\n",
        "  doc = TreebankWordDetokenizer().detokenize(lemm_sentence)\n",
        "  doc = nlp(doc)\n",
        "\n",
        "  for token in doc:\n",
        "    if token.pos_ == 'NOUN'or token.pos_ == 'VERB' or token.pos_ =='ADJ':\n",
        "      sentence_clean = sentence_clean + ' ' + token.text\n",
        "\n",
        "  similarity_score_Dcognomes = 0\n",
        "  similarity_score_tt = 0\n",
        "  list_words = []\n",
        "  doc = sentence_clean.split()\n",
        "\n",
        "  for i in doc:\n",
        "    if i in main_Dcognomes_dict.keys():\n",
        "      similarity_score_Dcognomes = similarity_score_Dcognomes + main_Dcognomes_dict[i]\n",
        "      \n",
        "    if i in main_dict.keys():\n",
        "      similarity_score_tt = similarity_score_tt + main_dict[i]\n",
        "\n",
        "    final_score = similarity_score_tt - similarity_score_Dcognomes\n",
        "\n",
        "    return  final_score"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va1Jt7C6-Mok"
      },
      "source": [
        "def Dcognomes_similarityscore(df1):\n",
        "  \n",
        "    df1['Nb_Dcognomes'] = df1['lemma'].apply(lambda x: Check_Dcognomes(x))\n",
        "    df1['score'] = df1['lemma'].apply(lambda x: count_DeceptiveCognome_and_similarity_score(x))\n",
        "\n",
        "    return df1\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieeGXjGvGfjG"
      },
      "source": [
        "df_clean = cleaning_Leff(df)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvWWBubC-gWz"
      },
      "source": [
        "df_clean_score = Dcognomes_similarityscore(df_clean)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Ub1eXSxIHnUE",
        "outputId": "e2aa9d89-50df-4bbc-83a4-db82bc1729f2"
      },
      "source": [
        "df_clean_score.head(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Level</th>\n",
              "      <th>Sentences</th>\n",
              "      <th>Cleaning1</th>\n",
              "      <th>Cleaning2</th>\n",
              "      <th>Cleaning3</th>\n",
              "      <th>Cleaning4</th>\n",
              "      <th>lemma</th>\n",
              "      <th>Cleaning7</th>\n",
              "      <th>Cleaning8</th>\n",
              "      <th>POS_LEFFF2</th>\n",
              "      <th>POS_without_lemma</th>\n",
              "      <th>Listes_verb</th>\n",
              "      <th>Listes_noun</th>\n",
              "      <th>Nb_Dcognomes</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B2</td>\n",
              "      <td>la formation s'achèvera par un contrôle pour s...</td>\n",
              "      <td>la formation se achèvera par un contrôle pour ...</td>\n",
              "      <td>la formation se achèvera par un contrôle pour ...</td>\n",
              "      <td>la formation se achèvera par un contrôle pour ...</td>\n",
              "      <td>formation achèvera par contrôle pour savoir pr...</td>\n",
              "      <td>formation achever par contrôle pour savoir pro...</td>\n",
              "      <td>formation achever contrôle propriétaire intégr...</td>\n",
              "      <td>formation achever contrôle propriétaire intégr...</td>\n",
              "      <td>{'formation': 'NOUN', 'achever': 'VERB', 'par'...</td>\n",
              "      <td>{'formation': 'NOUN', 'achèvera': 'VERB', 'par...</td>\n",
              "      <td>[achever, savoir]</td>\n",
              "      <td>[formation, contrôle, propriétaire, connaissance]</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C1</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives et des ordonnanc...</td>\n",
              "      <td>des sanctions administratives des ordonnances ...</td>\n",
              "      <td>un sanction administratif de ordonnance pénal ...</td>\n",
              "      <td>sanction administratif ordonnance pénal possib...</td>\n",
              "      <td>sanction administratif ordonnance pénal détent...</td>\n",
              "      <td>{'un': 'DET', 'sanction': 'NOUN', 'administrat...</td>\n",
              "      <td>{'des': 'DET', 'sanctions': 'NOUN', 'administr...</td>\n",
              "      <td>[renoncer, suivre, achever]</td>\n",
              "      <td>[sanction, ordonnance, détenteur, chien, forma...</td>\n",
              "      <td>0</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C1</td>\n",
              "      <td>le canidé, d'un âge avancé, a été bien malgré ...</td>\n",
              "      <td>le canidé, de un âge avancé, a été bien malgré...</td>\n",
              "      <td>le canidé de un âge avancé a été bien malgré l...</td>\n",
              "      <td>le canidé de un âge avancé a été bien malgré l...</td>\n",
              "      <td>canidé âge avancé été bien malgré lui coeur co...</td>\n",
              "      <td>canidé âg avancé être bien malgré lui coeur co...</td>\n",
              "      <td>canidé âg avancé malgré coeur commentaire déso...</td>\n",
              "      <td>canidé âg avancé coeur commentaire désobligean...</td>\n",
              "      <td>{'canidé': 'NOUN', 'âg': 'AUX', 'avancé': 'VER...</td>\n",
              "      <td>{'canidé': 'NOUN', 'âge': 'ADJ', 'avancé': 'AD...</td>\n",
              "      <td>[avancé, désobligeant, proférer, télévision]</td>\n",
              "      <td>[canidé, coeur, plateau, chaîne]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A2</td>\n",
              "      <td>tout savoir avant d'adopter un chat.</td>\n",
              "      <td>tout savoir avant de adopter un chat.</td>\n",
              "      <td>tout savoir avant de adopter un chat</td>\n",
              "      <td>tout savoir avant de adopter un chat</td>\n",
              "      <td>tout savoir avant adopter chat</td>\n",
              "      <td>tout savoir avant adopter chat</td>\n",
              "      <td>adopter chat</td>\n",
              "      <td>adopter chat</td>\n",
              "      <td>{'tout': 'PRON', 'savoir': 'VERB', 'avant': 'A...</td>\n",
              "      <td>{'tout': 'PRON', 'savoir': 'VERB', 'avant': 'A...</td>\n",
              "      <td>[savoir, adopter]</td>\n",
              "      <td>[chat]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2</td>\n",
              "      <td>le chat, le nouveau « meilleur ami de l'homme ».</td>\n",
              "      <td>le chat, le nouveau « meilleur ami de le homme ».</td>\n",
              "      <td>le chat le nouveau « meilleur ami de le homme »</td>\n",
              "      <td>le chat le nouveau « meilleur ami de le homme »</td>\n",
              "      <td>chat nouveau meilleur ami homme</td>\n",
              "      <td>chat nouveau meilleur ami homme</td>\n",
              "      <td>chat meilleur ami</td>\n",
              "      <td>chat meilleur ami</td>\n",
              "      <td>{'chat': 'NOUN', 'nouveau': 'ADJ', 'meilleur':...</td>\n",
              "      <td>{'chat': 'NOUN', 'nouveau': 'ADJ', 'meilleur':...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[chat, ami, homme]</td>\n",
              "      <td>0</td>\n",
              "      <td>86.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Level                                          Sentences  ... Nb_Dcognomes  score\n",
              "0    B2  la formation s'achèvera par un contrôle pour s...  ...            2  100.0\n",
              "1    C1  des sanctions administratives et des ordonnanc...  ...            0  100.0\n",
              "2    C1  le canidé, d'un âge avancé, a été bien malgré ...  ...            0    0.0\n",
              "3    A2               tout savoir avant d'adopter un chat.  ...            0    0.0\n",
              "4    A2   le chat, le nouveau « meilleur ami de l'homme ».  ...            0   86.0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMlYdI-MJYst"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwDK3AAVrPNc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "541bd230-7160-4910-d597-bb3459ecff64"
      },
      "source": [
        "def Counting_df(df):\n",
        "\n",
        "    def counting_by_category(sentences):                                 # Function that returns a dict{Part-of-speech: number of words contained} that composed each tweet\n",
        "      liste1 = []\n",
        "      doc = nlp(sentences)\n",
        "      for token in doc:\n",
        "          liste1.append(token.pos_)                         \n",
        "      return liste1\n",
        "\n",
        "    df['POS'] = df['Cleaning3'].apply(lambda x: counting_by_category(x))\n",
        "    df['counting'] = df['POS'].apply(lambda x: Counter(x))\n",
        "\n",
        "    feature_wordcount = pd.DataFrame(df['counting'])\n",
        "    feature_wordcount = feature_wordcount['counting'].apply(pd.Series)\n",
        "    feature_wordcount = feature_wordcount.fillna(0)\n",
        "    \n",
        "    return feature_wordcount\n",
        "\n",
        "Counting_df(df)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DET</th>\n",
              "      <th>NOUN</th>\n",
              "      <th>PRON</th>\n",
              "      <th>VERB</th>\n",
              "      <th>ADP</th>\n",
              "      <th>SCONJ</th>\n",
              "      <th>AUX</th>\n",
              "      <th>ADJ</th>\n",
              "      <th>CCONJ</th>\n",
              "      <th>ADV</th>\n",
              "      <th>PUNCT</th>\n",
              "      <th>NUM</th>\n",
              "      <th>SPACE</th>\n",
              "      <th>PROPN</th>\n",
              "      <th>X</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>18.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>17.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>22.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>26.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1030 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       DET  NOUN  PRON  VERB   ADP  SCONJ  ...   ADV  PUNCT  NUM  SPACE  PROPN    X\n",
              "0      4.0   4.0   1.0   3.0   2.0    1.0  ...   0.0    0.0  0.0    0.0    0.0  0.0\n",
              "1      3.0   5.0   1.0   3.0   4.0    1.0  ...   0.0    0.0  0.0    0.0    0.0  0.0\n",
              "2      4.0   7.0   1.0   1.0   7.0    0.0  ...   1.0    0.0  0.0    0.0    0.0  0.0\n",
              "3      1.0   1.0   1.0   2.0   2.0    0.0  ...   0.0    0.0  0.0    0.0    0.0  0.0\n",
              "4      3.0   3.0   0.0   0.0   1.0    0.0  ...   0.0    2.0  0.0    0.0    0.0  0.0\n",
              "...    ...   ...   ...   ...   ...    ...  ...   ...    ...  ...    ...    ...  ...\n",
              "1025  18.0  31.0  16.0  13.0  25.0    3.0  ...   9.0    0.0  1.0    0.0    0.0  0.0\n",
              "1026  17.0  22.0  19.0  12.0  23.0    6.0  ...  13.0    0.0  0.0    2.0    0.0  0.0\n",
              "1027  22.0  28.0  14.0  18.0  27.0    2.0  ...  13.0    0.0  0.0    1.0    0.0  0.0\n",
              "1028  26.0  34.0  12.0  20.0  32.0    2.0  ...   4.0    0.0  0.0    1.0    0.0  0.0\n",
              "1029  10.0  13.0  21.0  17.0  20.0    3.0  ...   7.0    0.0  0.0    1.0    0.0  0.0\n",
              "\n",
              "[1030 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "2Ex8ERAxTsA8",
        "outputId": "947c3e45-a55a-4547-b3cc-1242d5495c23"
      },
      "source": [
        "def final_df(df):\n",
        "  df_final = Counting_df(df).copy()\n",
        "  df_final['Level']= df['Level']\n",
        "  df_final['score'] = df['score']\n",
        "  df_final['D_cognome'] = df['Nb_Dcognomes']\n",
        "  return df_final\n",
        "\n",
        "\n",
        "final_df(df)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DET</th>\n",
              "      <th>NOUN</th>\n",
              "      <th>PRON</th>\n",
              "      <th>VERB</th>\n",
              "      <th>ADP</th>\n",
              "      <th>SCONJ</th>\n",
              "      <th>AUX</th>\n",
              "      <th>ADJ</th>\n",
              "      <th>CCONJ</th>\n",
              "      <th>ADV</th>\n",
              "      <th>PUNCT</th>\n",
              "      <th>NUM</th>\n",
              "      <th>SPACE</th>\n",
              "      <th>PROPN</th>\n",
              "      <th>X</th>\n",
              "      <th>Level</th>\n",
              "      <th>score</th>\n",
              "      <th>D_cognome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>B2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>C1</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>C1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>A2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>A2</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>18.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>C2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>17.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>C2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>22.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>C2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>26.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>C2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>C2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1030 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       DET  NOUN  PRON  VERB   ADP  ...  PROPN    X  Level  score  D_cognome\n",
              "0      4.0   4.0   1.0   3.0   2.0  ...    0.0  0.0     B2  100.0          2\n",
              "1      3.0   5.0   1.0   3.0   4.0  ...    0.0  0.0     C1  100.0          0\n",
              "2      4.0   7.0   1.0   1.0   7.0  ...    0.0  0.0     C1    0.0          0\n",
              "3      1.0   1.0   1.0   2.0   2.0  ...    0.0  0.0     A2    0.0          0\n",
              "4      3.0   3.0   0.0   0.0   1.0  ...    0.0  0.0     A2   86.0          0\n",
              "...    ...   ...   ...   ...   ...  ...    ...  ...    ...    ...        ...\n",
              "1025  18.0  31.0  16.0  13.0  25.0  ...    0.0  0.0     C2    0.0          4\n",
              "1026  17.0  22.0  19.0  12.0  23.0  ...    0.0  0.0     C2    0.0          4\n",
              "1027  22.0  28.0  14.0  18.0  27.0  ...    0.0  0.0     C2    0.0          0\n",
              "1028  26.0  34.0  12.0  20.0  32.0  ...    0.0  0.0     C2    0.0          2\n",
              "1029  10.0  13.0  21.0  17.0  20.0  ...    0.0  0.0     C2    0.0          0\n",
              "\n",
              "[1030 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0buzmxJiylTt"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I751HW_YH42H"
      },
      "source": [
        "def words_list(df):\n",
        "\n",
        "  common_words_cate = df.groupby(\"Level\")[\"Cleaning7\"].apply(lambda x: Counter(\" \".join(x).split()).most_common(100)).to_frame()\n",
        "\n",
        "  len_before = common_words_cate['Cleaning7'].apply(lambda x: len(x))\n",
        "\n",
        "  common_words_cate = common_words_cate.explode('Cleaning7')\n",
        "  common_words_cate['word'],common_words_cate['freq'] = common_words_cate.Cleaning7.str\n",
        "  common_words_cate['len_before'] = common_words_cate['word'].apply(lambda x: len(x))\n",
        "  common_words_cate = common_words_cate['word'].drop_duplicates().to_frame()\n",
        "  common_words_cate = common_words_cate.groupby('Level')['word'].apply(list).to_frame()\n",
        "\n",
        "  len_after = common_words_cate['word'].apply(lambda x: len(x))\n",
        "\n",
        "  final_df = pd.DataFrame()\n",
        "  final_df['len_before'] = len_before\n",
        "  final_df['len_after'] = len_after\n",
        "\n",
        "  return final_df\n",
        "\n",
        "words_list(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHxKUS5UdYsU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5vw5sNgBA-x"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df.groupby('Level')['Nb_Dcognomes'].mean().plot.bar(color = 'black')\n",
        "\n",
        "plt.title('Average by level of the number of deceptive cognates present in the sentences ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hNldGLWC6MX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}